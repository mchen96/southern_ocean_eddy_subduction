{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d94b9165-cdbd-4b39-8077-dd02e2448511",
   "metadata": {},
   "source": [
    "# Stratification Linked to Seasonality in Eddy Subduction in the Southern Ocean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fc0415-cf81-45e1-af59-ed5bf6a20100",
   "metadata": {},
   "source": [
    "Chen and Schofield, 2024\n",
    "\n",
    "Subduction anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419eacd7-340b-4d1a-87ec-eeed1bb29419",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64b57998-d6ec-4e72-b430-4541cfb8418a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy as sp\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "import gsw as gsw\n",
    "\n",
    "import cartopy as cp\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2376ecb-15d5-48f4-813b-5d1d333a9726",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import SOCCOM float data\n",
    "* 2023-08-28 Data snapshot downloaded from https://library.ucsd.edu/dc/collection/bb0488375t\n",
    "* Format: LIAR carbon algorithm, netCDF, low resolution \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cd27613-d12b-4952-9da7-e5d4d13ac6e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "floatspath = 'SOCCOM_GO-BGC_LoResQC_LIAR_28Aug2023_netcdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f00caf1-f2db-435f-9535-534d36d3c298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "floats = pd.read_csv('{}/MBARI_float_list.txt'.format(floatspath),sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5951c86d-abe7-46e5-9a74-ee677e21fcc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MBARI ID</th>\n",
       "      <th>INST ID</th>\n",
       "      <th>WMO</th>\n",
       "      <th>float type</th>\n",
       "      <th>1st lon</th>\n",
       "      <th>1st lat</th>\n",
       "      <th>1st date</th>\n",
       "      <th>msg dir</th>\n",
       "      <th>NC template</th>\n",
       "      <th>Program</th>\n",
       "      <th>...</th>\n",
       "      <th>tf NO3</th>\n",
       "      <th>tf pH</th>\n",
       "      <th>tf Chl</th>\n",
       "      <th>tf OCR</th>\n",
       "      <th>max cycle msg file</th>\n",
       "      <th>max cycle file date</th>\n",
       "      <th>latest cycle msg file</th>\n",
       "      <th>latest cycle file date</th>\n",
       "      <th>max cycle proc</th>\n",
       "      <th>max cycle proc date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ss0001</td>\n",
       "      <td>1</td>\n",
       "      <td>4903026</td>\n",
       "      <td>SOLO</td>\n",
       "      <td>-122.512</td>\n",
       "      <td>36.762</td>\n",
       "      <td>02/05/2022 03:42</td>\n",
       "      <td>\\\\seaecho.shore.mbari.org\\floats\\SIO\\ss0001\\</td>\n",
       "      <td>1</td>\n",
       "      <td>GO-BGC</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>231.0</td>\n",
       "      <td>08/09/2023 08:02</td>\n",
       "      <td>231.0</td>\n",
       "      <td>08/09/2023 08:02</td>\n",
       "      <td>231.0</td>\n",
       "      <td>08/09/2023 06:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ss0002</td>\n",
       "      <td>2</td>\n",
       "      <td>5906765</td>\n",
       "      <td>SOLO</td>\n",
       "      <td>-166.522</td>\n",
       "      <td>2.005</td>\n",
       "      <td>03/28/2022 17:31</td>\n",
       "      <td>\\\\seaecho.shore.mbari.org\\floats\\SIO\\ss0002\\</td>\n",
       "      <td>1</td>\n",
       "      <td>GO-BGC</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>95.0</td>\n",
       "      <td>08/10/2023 09:12</td>\n",
       "      <td>95.0</td>\n",
       "      <td>08/10/2023 09:12</td>\n",
       "      <td>95.0</td>\n",
       "      <td>08/10/2023 05:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ss0003</td>\n",
       "      <td>3</td>\n",
       "      <td>5906766</td>\n",
       "      <td>SOLO</td>\n",
       "      <td>-167.416</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>03/29/2022 03:46</td>\n",
       "      <td>\\\\seaecho.shore.mbari.org\\floats\\SIO\\ss0003\\</td>\n",
       "      <td>1</td>\n",
       "      <td>GO-BGC</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>06/26/2023 15:38</td>\n",
       "      <td>18.0</td>\n",
       "      <td>06/26/2023 15:38</td>\n",
       "      <td>18.0</td>\n",
       "      <td>04/05/2022 20:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ss0004</td>\n",
       "      <td>4</td>\n",
       "      <td>5906767</td>\n",
       "      <td>SOLO</td>\n",
       "      <td>-168.114</td>\n",
       "      <td>-1.602</td>\n",
       "      <td>03/29/2022 11:39</td>\n",
       "      <td>\\\\seaecho.shore.mbari.org\\floats\\SIO\\ss0004\\</td>\n",
       "      <td>1</td>\n",
       "      <td>GO-BGC</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>08/10/2023 08:04</td>\n",
       "      <td>70.0</td>\n",
       "      <td>08/10/2023 08:04</td>\n",
       "      <td>70.0</td>\n",
       "      <td>08/07/2023 23:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>un0037</td>\n",
       "      <td>37</td>\n",
       "      <td>5904475</td>\n",
       "      <td>NAVIS</td>\n",
       "      <td>11.352</td>\n",
       "      <td>-39.254</td>\n",
       "      <td>12/05/2014 22:05</td>\n",
       "      <td>\\\\seaecho.shore.mbari.org\\floats\\UW\\n0037\\</td>\n",
       "      <td>2</td>\n",
       "      <td>SOCCOM</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>03/17/2020 13:39</td>\n",
       "      <td>224.0</td>\n",
       "      <td>03/17/2020 13:39</td>\n",
       "      <td>224.0</td>\n",
       "      <td>03/17/2020 18:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>wn1485</td>\n",
       "      <td>1485</td>\n",
       "      <td>1902455</td>\n",
       "      <td>NAVIS</td>\n",
       "      <td>65.614</td>\n",
       "      <td>1.976</td>\n",
       "      <td>06/02/2023 14:23</td>\n",
       "      <td>\\\\seaecho.shore.mbari.org\\floats\\WHOI\\wn1485\\</td>\n",
       "      <td>6</td>\n",
       "      <td>GO-BGC</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>08/20/2023 21:05</td>\n",
       "      <td>9.0</td>\n",
       "      <td>08/20/2023 21:05</td>\n",
       "      <td>9.0</td>\n",
       "      <td>08/21/2023 03:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>wn1486</td>\n",
       "      <td>1486</td>\n",
       "      <td>1902458</td>\n",
       "      <td>NAVIS</td>\n",
       "      <td>67.849</td>\n",
       "      <td>12.471</td>\n",
       "      <td>06/14/2023 08:06</td>\n",
       "      <td>\\\\seaecho.shore.mbari.org\\floats\\WHOI\\wn1486\\</td>\n",
       "      <td>6</td>\n",
       "      <td>GO-BGC</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>08/22/2023 20:17</td>\n",
       "      <td>8.0</td>\n",
       "      <td>08/22/2023 20:17</td>\n",
       "      <td>8.0</td>\n",
       "      <td>08/23/2023 02:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>wn1487</td>\n",
       "      <td>1487</td>\n",
       "      <td>1902459</td>\n",
       "      <td>NAVIS</td>\n",
       "      <td>53.754</td>\n",
       "      <td>-27.843</td>\n",
       "      <td>04/21/2023 03:24</td>\n",
       "      <td>\\\\seaecho.shore.mbari.org\\floats\\WHOI\\wn1487\\</td>\n",
       "      <td>6</td>\n",
       "      <td>GO-BGC</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>08/28/2023 13:05</td>\n",
       "      <td>14.0</td>\n",
       "      <td>08/28/2023 13:05</td>\n",
       "      <td>13.0</td>\n",
       "      <td>08/18/2023 19:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>wn1488</td>\n",
       "      <td>1488</td>\n",
       "      <td>1902456</td>\n",
       "      <td>NAVIS</td>\n",
       "      <td>61.848</td>\n",
       "      <td>-27.044</td>\n",
       "      <td>05/09/2023 03:21</td>\n",
       "      <td>\\\\seaecho.shore.mbari.org\\floats\\WHOI\\wn1488\\</td>\n",
       "      <td>6</td>\n",
       "      <td>GO-BGC</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>08/26/2023 02:23</td>\n",
       "      <td>12.0</td>\n",
       "      <td>08/26/2023 02:23</td>\n",
       "      <td>12.0</td>\n",
       "      <td>08/26/2023 08:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>wn1490</td>\n",
       "      <td>1490</td>\n",
       "      <td>1902457</td>\n",
       "      <td>NAVIS</td>\n",
       "      <td>68.380</td>\n",
       "      <td>6.962</td>\n",
       "      <td>06/04/2023 01:34</td>\n",
       "      <td>\\\\seaecho.shore.mbari.org\\floats\\WHOI\\wn1490\\</td>\n",
       "      <td>6</td>\n",
       "      <td>GO-BGC</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>08/22/2023 11:07</td>\n",
       "      <td>9.0</td>\n",
       "      <td>08/22/2023 11:07</td>\n",
       "      <td>9.0</td>\n",
       "      <td>08/22/2023 17:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>505 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    MBARI ID  INST ID      WMO float type  1st lon  1st lat          1st date  \\\n",
       "0     ss0001        1  4903026       SOLO -122.512   36.762  02/05/2022 03:42   \n",
       "1     ss0002        2  5906765       SOLO -166.522    2.005  03/28/2022 17:31   \n",
       "2     ss0003        3  5906766       SOLO -167.416   -0.015  03/29/2022 03:46   \n",
       "3     ss0004        4  5906767       SOLO -168.114   -1.602  03/29/2022 11:39   \n",
       "4     un0037       37  5904475      NAVIS   11.352  -39.254  12/05/2014 22:05   \n",
       "..       ...      ...      ...        ...      ...      ...               ...   \n",
       "500   wn1485     1485  1902455      NAVIS   65.614    1.976  06/02/2023 14:23   \n",
       "501   wn1486     1486  1902458      NAVIS   67.849   12.471  06/14/2023 08:06   \n",
       "502   wn1487     1487  1902459      NAVIS   53.754  -27.843  04/21/2023 03:24   \n",
       "503   wn1488     1488  1902456      NAVIS   61.848  -27.044  05/09/2023 03:21   \n",
       "504   wn1490     1490  1902457      NAVIS   68.380    6.962  06/04/2023 01:34   \n",
       "\n",
       "                                           msg dir  NC template Program  ...  \\\n",
       "0     \\\\seaecho.shore.mbari.org\\floats\\SIO\\ss0001\\            1  GO-BGC  ...   \n",
       "1     \\\\seaecho.shore.mbari.org\\floats\\SIO\\ss0002\\            1  GO-BGC  ...   \n",
       "2     \\\\seaecho.shore.mbari.org\\floats\\SIO\\ss0003\\            1  GO-BGC  ...   \n",
       "3     \\\\seaecho.shore.mbari.org\\floats\\SIO\\ss0004\\            1  GO-BGC  ...   \n",
       "4       \\\\seaecho.shore.mbari.org\\floats\\UW\\n0037\\            2  SOCCOM  ...   \n",
       "..                                             ...          ...     ...  ...   \n",
       "500  \\\\seaecho.shore.mbari.org\\floats\\WHOI\\wn1485\\            6  GO-BGC  ...   \n",
       "501  \\\\seaecho.shore.mbari.org\\floats\\WHOI\\wn1486\\            6  GO-BGC  ...   \n",
       "502  \\\\seaecho.shore.mbari.org\\floats\\WHOI\\wn1487\\            6  GO-BGC  ...   \n",
       "503  \\\\seaecho.shore.mbari.org\\floats\\WHOI\\wn1488\\            6  GO-BGC  ...   \n",
       "504  \\\\seaecho.shore.mbari.org\\floats\\WHOI\\wn1490\\            6  GO-BGC  ...   \n",
       "\n",
       "    tf NO3  tf pH  tf Chl  tf OCR  max cycle msg file  max cycle file date  \\\n",
       "0        1      1       1       1               231.0     08/09/2023 08:02   \n",
       "1        1      1       1       1                95.0     08/10/2023 09:12   \n",
       "2        1      1       1       1                18.0     06/26/2023 15:38   \n",
       "3        1      1       1       1                70.0     08/10/2023 08:04   \n",
       "4        1      0       1       0               224.0     03/17/2020 13:39   \n",
       "..     ...    ...     ...     ...                 ...                  ...   \n",
       "500      1      1       1       0                 9.0     08/20/2023 21:05   \n",
       "501      1      1       1       0                 8.0     08/22/2023 20:17   \n",
       "502      1      1       1       0                14.0     08/28/2023 13:05   \n",
       "503      1      1       1       0                12.0     08/26/2023 02:23   \n",
       "504      1      1       1       0                 9.0     08/22/2023 11:07   \n",
       "\n",
       "     latest cycle msg file  latest cycle file date max cycle proc  \\\n",
       "0                    231.0        08/09/2023 08:02          231.0   \n",
       "1                     95.0        08/10/2023 09:12           95.0   \n",
       "2                     18.0        06/26/2023 15:38           18.0   \n",
       "3                     70.0        08/10/2023 08:04           70.0   \n",
       "4                    224.0        03/17/2020 13:39          224.0   \n",
       "..                     ...                     ...            ...   \n",
       "500                    9.0        08/20/2023 21:05            9.0   \n",
       "501                    8.0        08/22/2023 20:17            8.0   \n",
       "502                   14.0        08/28/2023 13:05           13.0   \n",
       "503                   12.0        08/26/2023 02:23           12.0   \n",
       "504                    9.0        08/22/2023 11:07            9.0   \n",
       "\n",
       "     max cycle proc date  \n",
       "0       08/09/2023 06:05  \n",
       "1       08/10/2023 05:53  \n",
       "2       04/05/2022 20:26  \n",
       "3       08/07/2023 23:11  \n",
       "4       03/17/2020 18:46  \n",
       "..                   ...  \n",
       "500     08/21/2023 03:46  \n",
       "501     08/23/2023 02:58  \n",
       "502     08/18/2023 19:25  \n",
       "503     08/26/2023 08:59  \n",
       "504     08/22/2023 17:30  \n",
       "\n",
       "[505 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56cf185f-6131-421a-952e-4bfa92fc55a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "so_floats = [] # A list of only floats that traveled <-30 latitude\n",
    "\n",
    "for i in floats.WMO.to_list():\n",
    "    if i in [i[-12:-5] for i in glob.glob('{}/{}QC.nc'.format(floatspath,i))]: # only floats that have QC nc files\n",
    "        db = xr.open_dataset('{}/{}QC.nc'.format(floatspath, i))\n",
    "        if db.Lat.min() <= -30:\n",
    "            so_floats.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aa1d3d0-ed3c-4faf-ac40-a0a6567ba7cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "so_floats.remove('5904982') # THIS FLOAT SEEMS TO HAVE A FAULTY OXYGEN SENSOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84068e62-a44c-46f7-bac8-29a11f1fcabb",
   "metadata": {},
   "source": [
    "## Differentiate between NAVIS vs Apex floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a3014f-8b63-4ddd-87f1-ebd343806fed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lists of different float types\n",
    "navis_floats = floats[floats['float type']=='NAVIS'].WMO.to_list()\n",
    "apex_floats = floats[floats['float type']=='APEX'].WMO.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86c71996-9564-4cc1-b88e-8cd5680f9f25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "solo = []\n",
    "apex = []\n",
    "navis = []\n",
    "\n",
    "for floatid in so_floats:\n",
    "    db = xr.open_dataset('{}/{}QC.nc'.format(floatspath,floatid))\n",
    "    depth_interval = np.ediff1d(db.Depth[0][np.where(db.Depth[0]<600)]).mean() # Average depth interval between 0-600 m\n",
    "\n",
    "    if floats[floats['WMO']==floatid]['float type'].item() == 'SOLO':\n",
    "        solo.append(depth_interval)\n",
    "    elif floats[floats['WMO']==floatid]['float type'].item() == 'APEX':\n",
    "        apex.append(depth_interval)\n",
    "    elif floats[floats['WMO']==floatid]['float type'].item() == 'NAVIS':\n",
    "        navis.append(depth_interval)\n",
    "\n",
    "#print('Median depth interval between 0-600 m:')\n",
    "#print('Navis floats: ',np.median(navis))\n",
    "#print('Apex floats: ',np.median(apex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "666e3e27-8123-456a-95c1-afc66e77fb44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median depth interval between 0-600 m:\n",
      "Navis floats:  -1.978734219269103\n",
      "Apex floats:  -11.534735294117647\n"
     ]
    }
   ],
   "source": [
    "solo = []\n",
    "apex = []\n",
    "navis = []\n",
    "\n",
    "for floatid in so_floats:\n",
    "    db = xr.open_dataset('{}/{}QC.nc'.format(floatspath, floatid))\n",
    "    depth_interval = np.ediff1d(db.Depth[0][np.where((db.Depth[0]<600)&(db.Depth[0]>0))]).mean() # Average depth interval between 0-600 m\n",
    "\n",
    "    if floats[floats['WMO']==floatid]['float type'].item() == 'SOLO':\n",
    "        solo.append(depth_interval)\n",
    "    elif floats[floats['WMO']==floatid]['float type'].item() == 'APEX':\n",
    "        apex.append(depth_interval)\n",
    "    elif floats[floats['WMO']==floatid]['float type'].item() == 'NAVIS':\n",
    "        navis.append(depth_interval)\n",
    "\n",
    "print('Median depth interval between 0-600 m:')\n",
    "print('Navis floats: ',np.median(navis))\n",
    "print('Apex floats: ',np.median(apex))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2811a8-958d-4a3f-b1c1-06f9565adb77",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38821de9-6285-4c1b-82ea-8fcb031fcd4e",
   "metadata": {},
   "source": [
    "### MLD: density threshold method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4851f94-7459-4c20-a6f8-0bdb924865aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mld05(dens,dens_QFA):\n",
    "    \"\"\"\n",
    "    Calculates depth pycnocline using the max of buoyancy frequency method\n",
    "     It calculates MLD per profile\n",
    "     It outputs depth_MLD, index_MLD and the Quality index\n",
    "     It requires as input: \n",
    "         1. density (2D matrix: N_PROF x N_LEVELS)\n",
    "    \"\"\"\n",
    "    \n",
    "    #===========================================\n",
    "    # Flag profiles that are >50% bad QC \n",
    "    num_invalid = np.count_nonzero(dens_QFA.isin([8]), axis=1) #Per profile (axis=1), count the number of readings with QC flags of 8 (bad). Returns an n-length array. Each value is the number of bad readings\n",
    "    half_levels = (np.count_nonzero(~np.isnan(dens), axis=1))/2 #Half the number of *valid* readings per profile\n",
    "    invalid_profiles = np.where(num_invalid/half_levels>0.5)[0] #The indices of profiles with >50% invalid readings\n",
    "    \n",
    "    #===========================================\n",
    "    # Initialize variables to hold output\n",
    "    MLD_n = np.empty(np.shape(dens)[0]) #Array to hold the calculated MLD for each profile (length N-profiles)\n",
    "    MLD_n[:] = np.nan # Initialize the array with nans\n",
    "    \n",
    "    #===========================================\n",
    "\n",
    "    indices_MLDs = np.zeros(dens.shape[0])+np.nan # Initialize an all-nan array the length of N_PROF (dens.shape[0])\n",
    "    \n",
    "    for i in range(len(dens)): # For each profile\n",
    "        #if dens[i][np.isnan(dens[i])==False][-1].Depth < 30: # If surface-most profile is shallower than 15 m deep (ignore profiles that didn't surface higher than that)\n",
    "        if i not in invalid_profiles and len(dens[i][~np.isnan(dens[i])])>0: # If the profile isn't all NaN\n",
    "            dens_prof = dens[i]  # Get just the current density profile\n",
    "            dens_prof = dens_prof[np.isnan(dens_prof)==False] # Remove NANs\n",
    "            \n",
    "            \n",
    "            surfacediff = dens_prof[:]-dens_prof[-1].item() # Calculate the difference from the surface density \n",
    "            if np.any(surfacediff>0.05): # Skip profiles where there's no density gradient > 0.05     \n",
    "                MLD = surfacediff.where(surfacediff>0.05,drop=True)[-1].Depth.item() # MLD is the shallowest depth where density difference from surface > 0.05 kg/m3\n",
    "                MLD_index = np.where(dens[i].Depth==MLD)[0][0] # Return the index of the MLD within the profile\n",
    "    \n",
    "                MLD_n[i] = MLD\n",
    "                indices_MLDs[i] = MLD_index\n",
    "                \n",
    "    return MLD_n, indices_MLDs\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49735cfe-3f64-4d69-8713-94b18a741fb9",
   "metadata": {},
   "source": [
    "### MLD: maxN2 (also returns maxN2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "715adcfa-5395-483c-bec6-74dc748614c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def profile2mld_n2(dens, P, Z, absal, ctemp, num_rows_ignore=0):\n",
    "    \"\"\"\n",
    "    Using the gsw package and TEOS-10, calculates MLD using the max of buoyancy frequency method\n",
    "     It calculates MLD per profile\n",
    "     It outputs depth_MLD, index_MLD\n",
    "     It requires as input: \n",
    "         1. density\n",
    "         2. depth\n",
    "         3. absolute salinity\n",
    "         4. conservative temperature\n",
    "         5. latitude\n",
    "         6. number of rows to ignore from surface to a determined depth - usually \n",
    "             due to high number of nans at the upper depths - max(N^2) is sometimes\n",
    "             max at the first non nan value. \n",
    "             use num_rows_ignore=0 tu use all data in the profile\n",
    "     In Matlab: Created by Filipa Carvalho. Written on June 29th, 2015. Last edited by Filipa on July 20th, 2015\n",
    "     Adapted for Python by Michael Chen.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #===========================================\n",
    "    # Flag profiles that have more than 50 nan points, meaning less than 50 actual values\n",
    "    num_nonnan = np.count_nonzero(~np.isnan(dens), axis=1) #Per profile (axis=1), count the number of non-nan (valid) readings. Returns an n-length array. Each value is the number of non-nan readings\n",
    "    half_levels = (np.shape(dens)[1])/2 #Half the number of depth levels. Profiles with > this number of nans will be flagged\n",
    "    nan_profiles = np.where(num_nonnan<=half_levels)[0] #The indices of profiles with <50% non-nan (valid) readings\n",
    "    \n",
    "    #===========================================\n",
    "    # Calculate Brunt Vaisala Frequency (N2)\n",
    "    BFRQI = gsw.Nsquared(absal, ctemp, P,lat=None,axis=1)[0] # returns an NxM-1 shaped matrix of n^2 values\n",
    "    # Append a n-length column of nans to beginning of BFRQI (i.e. the deepest depth. This will make BRFQI NxM0shaped again\n",
    "    new_col = np.empty((BFRQI.shape[0],1)) # Create a new column with the right shape (the number of profiles)\n",
    "    new_col[:] = np.nan # Fill it in with nans\n",
    "    BFRQI = np.c_[new_col,BFRQI] # Append it to the BFRQI array (at the deepest depth)\n",
    "    \n",
    "    \n",
    "    #===========================================\n",
    "    # Initialize variables to hold output\n",
    "    MLD_n = np.empty(np.shape(dens)[0]) #Array to hold the calculated MLD for each profile (length N-profiles)\n",
    "    MLD_n[:] = np.nan # Initialize the array with nans\n",
    "  \n",
    "    #QI_n(1:size(dens,2))=nan;\n",
    "    #max_n2(1:size(dens,2))=nan;\n",
    "    \n",
    "    #===========================================\n",
    "    # Grab profiles where max(N2) if valid \n",
    "    # Use the bfreq matrix to identify the index of the maximum in each profile\n",
    "    if num_rows_ignore == 0:\n",
    "        #index_max_n2 = np.nanargmax(BFRQI,axis=1) \n",
    "        \n",
    "        # Create an array with the indices of max n2. Ignore nans\n",
    "        index_max_n2 = np.zeros(BFRQI.shape[0])+np.nan # Initialize an all-nan array the length of N_PROF (BFRQI.shape[0])\n",
    "        max_n2 = np.nanmax(BFRQI,axis=1) # Calculate the max bfrq in each profile. This will return nan (and a warning) if the profile is empty\n",
    "        index_max_n2[~np.isnan(max_n2)] = np.nanargmax(BFRQI[~np.isnan(max_n2),:],axis=1) # At each instance where max_n2 is NOT nan, replace it with the proper index of the max bfrq\n",
    "        \n",
    "    else:\n",
    "        #index_max_n2 = np.nanargmax(BFRQI[:,:-num_rows_ignore],axis=1) \n",
    "        \n",
    "        # Retrieve an array with the indices of max n2. Ignore nans. Only calculate from rows that aren't ignored (shallow depths are at the end, so reverse index from the end)\n",
    "        index_max_n2 = np.zeros(BFRQI.shape[0])+np.nan # Initialize an all-nan array the length of N_PROF (BFRQI.shape[0])\n",
    "        max_n2 = np.nanmax(BFRQI[:,:-num_rows_ignore],axis=1) # Calculate the max bfrq in each profile. This will return nan (and a warning) if the profile is empty\n",
    "        index_max_n2[~np.isnan(max_n2)] = np.nanargmax(BFRQI[~np.isnan(max_n2),:-num_rows_ignore],axis=1) # At each instance where max_n2 is NOT nan, replace it with the proper index of the max bfrq\n",
    "        \n",
    "    #Use the indices of maximum bfreq to retrieve depth. This is the MLD.\n",
    "    for i in range(len(MLD_n)): #For every profile i\n",
    "        if i in nan_profiles or np.isnan(index_max_n2[i]): #Ignore profiles with >50% nans (defined above)\n",
    "            pass\n",
    "        else:\n",
    "            MLD_n[i] = Z[i,int(index_max_n2[i])] #Retrieve the depth (Z) in profile i at the index of max bfreq\n",
    "            \n",
    "    return MLD_n, index_max_n2, max_n2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b33c6ad-4a3c-4772-acd8-ff4125a4130a",
   "metadata": {},
   "source": [
    "## Drop floats with bad T, S, O data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97e793e5-257c-44d8-bcbc-824ef855b211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def IntersecOfSets(arr1, arr2, arr3):\n",
    "    # Converting the arrays into sets\n",
    "    s1 = set(arr1)\n",
    "    s2 = set(arr2)\n",
    "    s3 = set(arr3)\n",
    "     \n",
    "    # Calculates intersection of \n",
    "    # sets on s1 and s2\n",
    "    set1 = s1.intersection(s2)         #[80, 20, 100]\n",
    "     \n",
    "    # Calculates intersection of sets\n",
    "    # on set1 and s3\n",
    "    result_set = set1.intersection(s3)\n",
    "     \n",
    "    # Converts resulting set to list\n",
    "    final_list = list(result_set)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e2d87af-c495-4460-a3d5-7e49a721f00e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bad_oxygen = []\n",
    "bad_t = []\n",
    "bad_s = []\n",
    "\n",
    "for floatid in so_floats:\n",
    "    db = xr.open_dataset('{}/{}QC.nc'.format(floatspath,floatid))\n",
    "    \n",
    "    for i in range(len(db.N_PROF)):\n",
    "        if len(np.where(db.Oxygen_QF.values[i]==b'0')[0]) < (0.9* len(db.Oxygen_QF.values[i])):\n",
    "               bad_oxygen.append((floatid,i))\n",
    "        if len(np.where(db.Temperature_QF.values[i]==b'0')[0]) < (0.9* len(db.Temperature_QF.values[i])):\n",
    "               bad_t.append((floatid,i))\n",
    "        if len(np.where(db.Salinity_QF.values[i]==b'0')[0]) < (0.9 * len(db.Salinity_QF.values[i])):\n",
    "               bad_s.append((floatid,i))\n",
    "\n",
    "bad_tso = IntersecOfSets(bad_oxygen,bad_t,bad_s)\n",
    "\n",
    "bad_oxygen_db = pd.DataFrame(bad_oxygen,columns=['floatid','N_PROF'])\n",
    "bad_t_db = pd.DataFrame(bad_t,columns=['floatid','N_PROF'])\n",
    "bad_s_db = pd.DataFrame(bad_s,columns=['floatid','N_PROF'])\n",
    "bad_tso_db = pd.DataFrame(bad_tso,columns=['floatid','N_PROF'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfc16a80-9faf-48a9-aa97-3fc4ee9e054b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>floatid</th>\n",
       "      <th>N_PROF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5906205</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5905105</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5904765</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5904475</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5905131</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>5906036</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>5904470</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852</th>\n",
       "      <td>5905105</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853</th>\n",
       "      <td>5905995</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1854</th>\n",
       "      <td>5904468</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1855 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      floatid  N_PROF\n",
       "0     5906205      64\n",
       "1     5905105     213\n",
       "2     5904765       9\n",
       "3     5904475     171\n",
       "4     5905131       1\n",
       "...       ...     ...\n",
       "1850  5906036      15\n",
       "1851  5904470      79\n",
       "1852  5905105     177\n",
       "1853  5905995     137\n",
       "1854  5904468      56\n",
       "\n",
       "[1855 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_tso_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e851a5aa-5087-4953-ab80-1a039f30cc50",
   "metadata": {},
   "source": [
    "## Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaa5310-cee8-4046-ba42-64acb536e638",
   "metadata": {},
   "source": [
    "Algorithm based on Chen et al, 2021; Llort et al, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "10e619ca-905a-42be-b79d-8275b71fdf54",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5904475\n",
      "5903717\n",
      "5904670\n",
      "5904671\n",
      "5904476\n",
      "5904477\n",
      "5904686\n",
      "5904478\n",
      "5904687\n",
      "5904672\n",
      "5904766\n",
      "5904983\n",
      "5904767\n",
      "5904984\n",
      "5904768\n",
      "5904673\n",
      "5904846\n",
      "5904847\n",
      "5905071\n",
      "5905072\n",
      "5905073\n",
      "5905074\n",
      "5905996\n",
      "5906304\n",
      "5906035\n",
      "5906036\n",
      "5906305\n",
      "5906306\n",
      "5906307\n",
      "5906308\n",
      "5906309\n",
      "5906508\n",
      "5906509\n",
      "5906442\n",
      "5906443\n",
      "5906444\n",
      "5906310\n",
      "5906311\n",
      "5906566\n",
      "5906538\n",
      "5901492\n",
      "5902112\n",
      "5904179\n",
      "5903612\n",
      "5903718\n",
      "5903593\n",
      "5903755\n",
      "5904181\n",
      "5904182\n",
      "5904180\n",
      "5904183\n",
      "5904105\n",
      "5904104\n",
      "5904467\n",
      "5905109\n",
      "5905075\n",
      "5904470\n",
      "5904186\n",
      "5904396\n",
      "5904184\n",
      "5904185\n",
      "5904471\n",
      "5904188\n",
      "5904469\n",
      "5904468\n",
      "5904187\n",
      "5904397\n",
      "5904395\n",
      "5904473\n",
      "5904695\n",
      "5904472\n",
      "5904474\n",
      "5904688\n",
      "5904684\n",
      "5904674\n",
      "5904677\n",
      "5904763\n",
      "5904693\n",
      "5904682\n",
      "5904685\n",
      "5904676\n",
      "5904661\n",
      "5904683\n",
      "5904660\n",
      "5904658\n",
      "5904659\n",
      "5904843\n",
      "5904761\n",
      "5904657\n",
      "5904662\n",
      "5904663\n",
      "5904678\n",
      "5904675\n",
      "5904845\n",
      "5904694\n",
      "5904679\n",
      "5904764\n",
      "5904765\n",
      "5904844\n",
      "5905995\n",
      "5905100\n",
      "5905102\n",
      "5905080\n",
      "5905106\n",
      "5905103\n",
      "5905078\n",
      "5905107\n",
      "5905101\n",
      "5905077\n",
      "5905108\n",
      "5904857\n",
      "5904841\n",
      "5905076\n",
      "5905099\n",
      "5904981\n",
      "5905104\n",
      "5905638\n",
      "5905381\n",
      "5905070\n",
      "5905105\n",
      "5904860\n",
      "5905079\n",
      "5904980\n",
      "5904856\n",
      "5904859\n",
      "5904858\n",
      "5904842\n",
      "5905069\n",
      "5904855\n",
      "5904854\n",
      "5905382\n",
      "5905383\n",
      "5905991\n",
      "5905983\n",
      "5905635\n",
      "5905379\n",
      "5906003\n",
      "5906001\n",
      "5905375\n",
      "5906000\n",
      "5905380\n",
      "5905997\n",
      "5905134\n",
      "5905998\n",
      "5905133\n",
      "5905369\n",
      "5905131\n",
      "5905370\n",
      "5905376\n",
      "5905994\n",
      "5905377\n",
      "5905135\n",
      "5905130\n",
      "5905132\n",
      "5905984\n",
      "5905378\n",
      "5905992\n",
      "5906004\n",
      "5905636\n",
      "5905367\n",
      "5905366\n",
      "5905637\n",
      "5905634\n",
      "5905373\n",
      "5905985\n",
      "5905371\n",
      "5905368\n",
      "5905374\n",
      "5905372\n",
      "5906005\n",
      "5905639\n",
      "5906033\n",
      "5906008\n",
      "5906007\n",
      "5905982\n",
      "5906032\n",
      "5906006\n",
      "5906031\n",
      "5905993\n",
      "5906030\n",
      "5906002\n",
      "5906034\n",
      "5906551\n",
      "5906315\n",
      "5906208\n",
      "5906207\n",
      "5906250\n",
      "5906218\n",
      "5906205\n",
      "5906228\n",
      "5906215\n",
      "5906219\n",
      "5906247\n",
      "5906227\n",
      "5906214\n",
      "5906204\n",
      "5906213\n",
      "5906249\n",
      "5906220\n",
      "5906246\n",
      "5906248\n",
      "5906216\n",
      "5906224\n",
      "5906244\n",
      "5906212\n",
      "5906209\n",
      "5906245\n",
      "5906217\n",
      "5906221\n",
      "5906314\n",
      "5906206\n",
      "5906226\n",
      "5906222\n",
      "5906225\n",
      "5906223\n",
      "5906211\n",
      "5906210\n",
      "5906501\n",
      "5906318\n",
      "5906495\n",
      "5906317\n",
      "5906319\n",
      "5906316\n",
      "5906312\n",
      "5906546\n",
      "5906500\n",
      "5906544\n",
      "5906313\n",
      "5906545\n",
      "5906493\n",
      "5906488\n",
      "5906499\n",
      "5906498\n",
      "5906496\n",
      "5906487\n",
      "5906494\n",
      "5906489\n",
      "5906562\n",
      "5906497\n",
      "5906528\n",
      "5906568\n",
      "5906550\n",
      "5906541\n",
      "2903458\n",
      "5906526\n",
      "5906582\n",
      "5906554\n",
      "5906491\n",
      "5906553\n",
      "5906580\n",
      "5906542\n",
      "5906579\n",
      "5906567\n",
      "7900826\n",
      "5906556\n",
      "5906557\n",
      "5906492\n",
      "5906524\n",
      "5906527\n",
      "5906552\n",
      "5906525\n",
      "5906490\n",
      "5906569\n",
      "2903457\n",
      "5906559\n",
      "5906558\n",
      "5906583\n",
      "5906560\n",
      "2903455\n",
      "2903454\n",
      "5906561\n",
      "2903456\n",
      "7900827\n",
      "7900825\n",
      "7900824\n",
      "7900823\n",
      "2903453\n",
      "5906581\n",
      "2903854\n"
     ]
    }
   ],
   "source": [
    "# Derive variables using TEOS-10 and the gsw package in python\n",
    "\n",
    "anoms = [] # Dataframe to store anomalies. Define anomaly at the depth of the maximum AOU anomaly (i.e. spice anomaly defined at that depth, not at the depth of the max spice anomaly)\n",
    "lons_all = []\n",
    "lats_all = []\n",
    "T_all = []\n",
    "S_all = [] \n",
    "depth_all = [] \n",
    "floatid_all = []\n",
    "station_all = []\n",
    "surface_bgc = [] # Dataframe to store surface biogeochemical info for *all* considered profiles\n",
    "MLD_method = 'dens05'\n",
    "apex_schedule = np.concatenate((np.arange(5,105,5),np.arange(110,370,10),np.arange(380,400,20),np.arange(400,1010,50))) # List of APEX sampling depths\n",
    "\n",
    "for floatid in so_floats:\n",
    "    print(floatid)\n",
    "    db = xr.open_dataset('{}/{}QC.nc'.format(floatspath,floatid))\n",
    "    \n",
    "    # Assign Depth as a coordinate, which will allow integration by depth\n",
    "    db = db.set_coords('Depth')\n",
    "    \n",
    "    # Assign relevant parameters to variables\n",
    "    S = db.Salinity.copy(deep=True)\n",
    "    T = db.Temperature.copy(deep=True)\n",
    "    P = db.Pressure.copy(deep=True)\n",
    "    Z = db.Depth.copy(deep=True)\n",
    "    depths = Z.copy(deep=True)\n",
    "    dens = db.Sigma_theta.copy(deep=True)\n",
    "    dens_QFA = db.Sigma_theta_QFA.copy(deep=True)\n",
    "    lon2d = np.repeat(db.Lon.values, len(db.N_LEVELS)).reshape(-1, len(db.N_LEVELS)) #reshape lon and lat into 2d arrays to input to absolute salinity function\n",
    "    lat2d = np.repeat(db.Lat.values, len(db.N_LEVELS)).reshape(-1, len(db.N_LEVELS))\n",
    "    \n",
    "    # Check that all profiles have the same dimensions\n",
    "    assert(np.shape(S) == np.shape(T) == np.shape(P) == np.shape(Z) == np.shape(dens))\n",
    "    \n",
    "    # Derive variables using TEOS-10 and the GSW package\n",
    "    # AOU calculated using oxygen solubility from Garcia and Gordon (1992)\n",
    "    # Spice calculated according to McDougall and Krzysik (2015)\n",
    "    # MLDs calculated according to Carvalho et al (2017)\n",
    "   \n",
    "    absal = gsw.SA_from_SP(S, P, lon2d, lat2d)\n",
    "    ctemp = gsw.CT_from_t(absal,T,P)\n",
    "    ptemp = gsw.pt_from_t(absal,T,P,p_ref=0)\n",
    "    sigmagsw = gsw.sigma0(absal,ctemp) # GSW calculated sigma: ref pressure 0dar\n",
    "    spice = gsw.spiciness0(absal,ctemp) # Ref pressure 0db: \"Calculates spiciness from Absolute Salinity and Conservative Temperature at a pressure of 0 dbar, as described by McDougall and Krzysik (2015).\"\n",
    "    o2sol = gsw.O2sol(absal,ctemp,P,lon2d,lat2d) # \"solubility coefficients derived from the data of Benson and Krause (1984), as fitted by Garcia and Gordon (1992, 1993).\"\n",
    "    aou = o2sol - db.Oxygen.data # AOU = expected - observed\n",
    "    \n",
    "    # Add as data variables to the dataframe\n",
    "    db[\"Spice\"] = (['N_PROF', 'N_LEVELS'], spice.data)\n",
    "    db[\"Absolute_Salinity\"] = (['N_PROF', 'N_LEVELS'], absal.data)\n",
    "    db[\"AOU\"] = (['N_PROF', 'N_LEVELS'], aou.data)\n",
    "    db[\"Conservative_Temp\"] = (['N_PROF', 'N_LEVELS'], ctemp.data)\n",
    "    db[\"Potential_Temp\"] = (['N_PROF', 'N_LEVELS'], ptemp.data)\n",
    "    db[\"Sigma0_GSW\"] = (['N_PROF', 'N_LEVELS'], sigmagsw.data)\n",
    "    \n",
    "    if floatid in navis_floats:\n",
    "        ######### DOWNSAMPLE NAVIS FLOATS\n",
    "        #### Define the N_LEVELS depth indices across the float dataset that correspond to APEX sampling depths \n",
    "        #### Store indices in matrix downsampled_i\n",
    "        #### Mark indices where there is no matchup in downsampled_i_fornan; downsampled data here will be marked nan\n",
    "        #### N = N_PROF\n",
    "        #### M = N_LEVELS\n",
    "        downsampled_i = [] # NxM matrix of N_LEVELS indices for each profile at APEX depths\n",
    "        downsampled_i_fornan = [ [] for _ in range(len(db.N_PROF))] # Nx0 matrix to hold indices where the Navis float did not sample at the given APEX depth\n",
    "        \n",
    "        for i in range(len(apex_schedule)): # For each APEX sampling depth\n",
    "            d = apex_schedule[i] \n",
    "            \n",
    "            diff_array = np.absolute(db.Depth-d) # NxM Difference array of the full depth dataset vs the target APEX depth\n",
    "            diff_indices = diff_array.argmin(axis=1) #Nx1 the indices of the nearest depth in each profile (minimum difference)\n",
    "            \n",
    "            nearest_depths = db.Depth[:,diff_indices] # Nx1 retrieve the observed depths at those indices in each profile\n",
    "            nearest_depths_diffs = np.absolute(nearest_depths-d) # Nx1 difference array of those indices versus the target depth (d)\n",
    "            \n",
    "            if d<=100:\n",
    "                no_meas = np.where(nearest_depths_diffs>2.5)[0] # N_PROF Indices where the closest sampled depth was >2.5 m away from target depth (>halfway between 5 m sampling interval)\n",
    "            elif d>100 and d<=360:\n",
    "                no_meas = np.where(nearest_depths_diffs>5)[0] # N_PROF Indices where the closest sampled depth was >5 m away from target depth (>halfway between 10 m sampling interval)\n",
    "            elif d>360 and d<=400:\n",
    "                no_meas = np.where(nearest_depths_diffs>10)[0] # N_PROF Indices where the closest sampled depth was >10 m away from target depth (>halfway between 20 m sampling interval)\n",
    "            elif d>400 and d<=1000:\n",
    "                no_meas = np.where(nearest_depths_diffs>25)[0] # N_PROF Indices where the closest sampled depth was >25 m away from target depth (>halfway between 50 m sampling interval)\n",
    "            \n",
    "            for nprof in no_meas:\n",
    "                downsampled_i_fornan[nprof].extend([59-i]) # Record the N_LEVEL indices for this N_PROF that are NaN\n",
    "            \n",
    "            downsampled_i.append(list(diff_indices.values))\n",
    "            \n",
    "            \n",
    "        downsampled_i = np.flip(np.transpose(downsampled_i),axis=1) #  Transpose the matrix from MxN and shallow->deep, into NxM and deep->shallow (standard float data format)\n",
    "    \n",
    "        \n",
    "        #########################\n",
    "        ###### Use the downsample indices to downsample variable arrays\n",
    "        \n",
    "        if len(db.N_PROF)==1 and len(downsampled_i)==1:\n",
    "            depths_ds = db.Depth[:,downsampled_i[0]]\n",
    "            P_ds = db.Pressure[:,downsampled_i[0]]\n",
    "            dens_ds = db.Sigma_theta[:,downsampled_i[0]]\n",
    "            dens_QFA_ds = db.Sigma_theta_QFA[:,downsampled_i[0]]\n",
    "            T_ds = db.Conservative_Temp[:,downsampled_i[0]]\n",
    "            S_ds = db.Absolute_Salinity[:,downsampled_i[0]]\n",
    "            sigmagsw_ds = db.Sigma0_GSW[:,downsampled_i[0]]\n",
    "            spice_ds = db.Spice[:,downsampled_i[0]]\n",
    "            aou_ds = db.AOU[:,downsampled_i[0]]\n",
    "            chl_ds = db.Chl_a[:,downsampled_i[0]]\n",
    "            bbp_ds = db.b_bp700[:,downsampled_i[0]]\n",
    "            poc_ds = db.POC[:,downsampled_i[0]]\n",
    "            \n",
    "        elif len(db.N_PROF) > 1:\n",
    "            # Initialize downsampled arrays for each variable by indexing the first profile at just the APEX depths\n",
    "            depths_ds = db.Depth[0,downsampled_i[0]]\n",
    "            P_ds = db.Pressure[0,downsampled_i[0]]\n",
    "            dens_ds = db.Sigma_theta[0,downsampled_i[0]]\n",
    "            dens_QFA_ds = db.Sigma_theta_QFA[0,downsampled_i[0]]\n",
    "            T_ds = db.Conservative_Temp[0,downsampled_i[0]]\n",
    "            S_ds = db.Absolute_Salinity[0,downsampled_i[0]]\n",
    "            sigmagsw_ds = db.Sigma0_GSW[0,downsampled_i[0]]\n",
    "            spice_ds = db.Spice[0,downsampled_i[0]]\n",
    "            aou_ds = db.AOU[0,downsampled_i[0]]\n",
    "            chl_ds = db.Chl_a[0,downsampled_i[0]]\n",
    "            bbp_ds = db.b_bp700[0,downsampled_i[0]]\n",
    "            poc_ds = db.POC[0,downsampled_i[0]]\n",
    "            \n",
    "            # Replace unsampled depths with NAN (at the N_LEVELS indices recorded earlier)\n",
    "            depths_ds[downsampled_i_fornan[0]] = np.nan\n",
    "            P_ds[downsampled_i_fornan[0]] = np.nan\n",
    "            dens_ds[downsampled_i_fornan[0]] = np.nan\n",
    "            dens_QFA_ds[downsampled_i_fornan[0]] = np.nan\n",
    "            T_ds[downsampled_i_fornan[0]] = np.nan\n",
    "            S_ds[downsampled_i_fornan[0]] = np.nan\n",
    "            sigmagsw_ds[downsampled_i_fornan[0]] = np.nan\n",
    "            spice_ds[downsampled_i_fornan[0]] = np.nan\n",
    "            aou_ds[downsampled_i_fornan[0]] = np.nan\n",
    "            chl_ds[downsampled_i_fornan[0]] = np.nan\n",
    "            bbp_ds[downsampled_i_fornan[0]] = np.nan\n",
    "            poc_ds[downsampled_i_fornan[0]] = np.nan\n",
    "            \n",
    "            # Downsample each variable by indexing each profile, substituting NANs, and then concatenating with the downsampled data array\n",
    "            for i in range(1,len(downsampled_i)):\n",
    "                depths_ds_prof = db.Depth[i,downsampled_i[i]]\n",
    "                P_ds_prof = db.Pressure[i,downsampled_i[i]]\n",
    "                dens_ds_prof = db.Sigma_theta[i,downsampled_i[i]]\n",
    "                dens_QFA_ds_prof = db.Sigma_theta_QFA[i,downsampled_i[i]]\n",
    "                T_ds_prof = db.Conservative_Temp[i,downsampled_i[i]]\n",
    "                S_ds_prof = db.Absolute_Salinity[i,downsampled_i[i]]\n",
    "                sigmagsw_ds_prof = db.Sigma0_GSW[i,downsampled_i[i]]\n",
    "                spice_ds_prof = db.Spice[i,downsampled_i[i]]\n",
    "                aou_ds_prof = db.AOU[i,downsampled_i[i]]\n",
    "                chl_ds_prof = db.Chl_a[i,downsampled_i[i]]\n",
    "                bbp_ds_prof = db.b_bp700[i,downsampled_i[i]]\n",
    "                poc_ds_prof = db.POC[i,downsampled_i[i]]\n",
    "                    \n",
    "                depths_ds_prof[downsampled_i_fornan[i]] = np.nan\n",
    "                P_ds_prof[downsampled_i_fornan[i]] = np.nan\n",
    "                dens_ds_prof[downsampled_i_fornan[i]] = np.nan\n",
    "                dens_QFA_ds_prof[downsampled_i_fornan[i]] = np.nan\n",
    "                T_ds_prof[downsampled_i_fornan[i]] = np.nan\n",
    "                S_ds_prof[downsampled_i_fornan[i]] = np.nan\n",
    "                sigmagsw_ds_prof[downsampled_i_fornan[i]] = np.nan\n",
    "                spice_ds_prof[downsampled_i_fornan[i]] = np.nan\n",
    "                aou_ds_prof[downsampled_i_fornan[i]] = np.nan\n",
    "                chl_ds_prof[downsampled_i_fornan[i]] = np.nan\n",
    "                bbp_ds_prof[downsampled_i_fornan[i]] = np.nan\n",
    "                poc_ds_prof[downsampled_i_fornan[i]] = np.nan\n",
    "                    \n",
    "                depths_ds = xr.concat([depths_ds, depths_ds_prof], dim=\"N_PROF\")\n",
    "                P_ds = xr.concat([P_ds, P_ds_prof], dim=\"N_PROF\")\n",
    "                dens_ds = xr.concat([dens_ds, dens_ds_prof], dim=\"N_PROF\")\n",
    "                dens_QFA_ds = xr.concat([dens_QFA_ds, dens_QFA_ds_prof], dim=\"N_PROF\")\n",
    "                T_ds = xr.concat([T_ds, T_ds_prof], dim=\"N_PROF\")\n",
    "                S_ds = xr.concat([S_ds, S_ds_prof], dim=\"N_PROF\")\n",
    "                sigmagsw_ds = xr.concat([sigmagsw_ds, sigmagsw_ds_prof], dim=\"N_PROF\")\n",
    "                spice_ds = xr.concat([spice_ds, spice_ds_prof], dim=\"N_PROF\")\n",
    "                aou_ds = xr.concat([aou_ds, aou_ds_prof], dim=\"N_PROF\")\n",
    "                chl_ds = xr.concat([chl_ds, chl_ds_prof], dim=\"N_PROF\")\n",
    "                bbp_ds = xr.concat([bbp_ds, bbp_ds_prof], dim=\"N_PROF\")\n",
    "                poc_ds = xr.concat([poc_ds, poc_ds_prof], dim=\"N_PROF\")\n",
    "        \n",
    "        depths = depths_ds.copy(deep=True) # For code reuseability with APEX floats, now with the truncated shape of the downsampled arrays\n",
    "        dens = dens_ds.copy(deep=True)\n",
    "    #########################\n",
    "    #########################\n",
    "    \n",
    "    if MLD_method == 'maxn2':\n",
    "        if floatid in apex_floats:\n",
    "            MLD, indicesMLD, maxn2 = profile2mld_n2(dens, P, Z, absal, ctemp, num_rows_ignore=0)\n",
    "        elif floatid in navis_floats:\n",
    "            MLD, indicesMLD, maxn2 = profile2mld_n2(dens_ds, P_ds, depths_ds, S_ds, T_ds, num_rows_ignore=0)\n",
    "    \n",
    "    elif MLD_method == 'dens05':\n",
    "        if floatid in apex_floats:\n",
    "            MLD, indicesMLD = mld05(dens,dens_QFA)\n",
    "            MLD_maxn2, indicesMLD_maxn2, maxn2 = profile2mld_n2(dens, P, Z, absal, ctemp, num_rows_ignore=0)\n",
    "        elif floatid in navis_floats:\n",
    "            MLD, indicesMLD = mld05(dens_ds,dens_QFA_ds)\n",
    "            MLD_navis, indicesMLD_navis = mld05(db.Sigma_theta,db.Sigma_theta_QFA)\n",
    "            MLD_maxn2, indicesMLD_maxn2, maxn2 = profile2mld_n2(dens_ds, P_ds, depths_ds, S_ds, T_ds, num_rows_ignore=0)\n",
    "    \n",
    "    # Add as data variables to the dataframe\n",
    "    db[\"Mixed_Layer_Depth\"] = (['N_PROF'], MLD)\n",
    "    db['Max_N2'] = ('N_PROF',maxn2)\n",
    "    \n",
    "    #########################\n",
    "    # Calculate 3-bin rolling medians for variables of interest. Don't drop nans.\n",
    "    # For Navis floats, use the downsampled data\n",
    "    \n",
    "    if floatid in apex_floats:\n",
    "        \n",
    "        depth_bin3 = db.Depth.rolling(N_LEVELS=3,center=True,min_periods=2).median()\n",
    "        T_bin3 = db.Conservative_Temp.rolling(N_LEVELS=3,center=True,min_periods=2).median()\n",
    "        S_bin3 = db.Absolute_Salinity.rolling(N_LEVELS=3,center=True,min_periods=2).median()\n",
    "        sigmagsw_bin3 = db.Sigma0_GSW.rolling(N_LEVELS=3,center=True,min_periods=2).median()\n",
    "        spicebin3 = db.Spice.rolling(N_LEVELS=3,center=True,min_periods=2).median()\n",
    "        aou_bin3 = db.AOU.rolling(N_LEVELS=3,center=True,min_periods=2).median()\n",
    "        bbp_bin3 = db.b_bp700.rolling(N_LEVELS=3,center=True,min_periods=2).median()\n",
    "        chl_bin3 = db.Chl_a.rolling(N_LEVELS=3,center=True,min_periods=2).median()\n",
    "        poc_bin3 = db.POC.rolling(N_LEVELS=3,center=True,min_periods=2).median()\n",
    "        \n",
    "        bbp_despiked = np.nan\n",
    "        bbp_spikes = np.nan\n",
    "        chl_despiked = np.nan\n",
    "        chl_spikes = np.nan\n",
    "        poc_despiked = np.nan\n",
    "        poc_spikes = np.nan\n",
    "    \n",
    "    elif floatid in navis_floats: # Use the downsampled data for rolling medians for Navis data\n",
    "        \n",
    "        depth_bin3 = depths.rolling(N_LEVELS=3,center=True,min_periods=2).median()\n",
    "        T_bin3 = T_ds.rolling(N_LEVELS=3,center=True,min_periods=2).median()\n",
    "        S_bin3 = S_ds.rolling(N_LEVELS=3,center=True,min_periods=2).median()\n",
    "        sigmagsw_bin3 = sigmagsw_ds.rolling(N_LEVELS=3,center=True,min_periods=2).median()\n",
    "        spicebin3 = spice_ds.rolling(N_LEVELS=3,center=True,min_periods=2).median()\n",
    "        aou_bin3 = aou_ds.rolling(N_LEVELS=3,center=True,min_periods=2).median()\n",
    "        bbp_bin3 = bbp_ds.rolling(N_LEVELS=3,center=True,min_periods=2).median()\n",
    "        chl_bin3 = chl_ds.rolling(N_LEVELS=3,center=True,min_periods=2).median()\n",
    "        poc_bin3 = poc_ds.rolling(N_LEVELS=3,center=True,min_periods=2).median()\n",
    "        \n",
    "        ################ For Navis floats, also perform a spike analysis. Navis floats have higher vertical resolution (2m)\n",
    "        # Following Rembauville et al 2017 and Lacour et al (2019):\n",
    "        # Despike by: Applyying a 5-bin running median filter followed by a 7-bin running mean filter\n",
    "        # Subtract the despiked data from the raw data\n",
    "        bbp_despiked = db.b_bp700.rolling(N_LEVELS=5,center=True,min_periods=3).median()\n",
    "        bbp_despiked = bbp_despiked.rolling(N_LEVELS=7,center=True,min_periods=3).mean()\n",
    "        bbp_spikes = db.b_bp700 - bbp_despiked\n",
    "        \n",
    "        chl_despiked = db.Chl_a.rolling(N_LEVELS=5,center=True,min_periods=3).median()\n",
    "        chl_despiked = chl_despiked.rolling(N_LEVELS=7,center=True,min_periods=3).mean()\n",
    "        chl_spikes = db.Chl_a - chl_despiked\n",
    "        \n",
    "        poc_despiked = db.POC.rolling(N_LEVELS=5,center=True,min_periods=3).median()\n",
    "        poc_despiked = poc_despiked.rolling(N_LEVELS=7,center=True,min_periods=3).mean()\n",
    "        poc_spikes = db.POC - poc_despiked\n",
    "    \n",
    "    # Calculate the chl/bbp ratio by dividing the 3-bin smoothed profiles\n",
    "    chl_bbp_ratio3 = chl_bin3/bbp_bin3\n",
    "    \n",
    "    ########### Create empty variables for reference profiles of variables of interest. Use these just to input the points used to calculate the reference profile (above and below an anomaly).\n",
    "    ####### By integrating through these points (straight line), I will calculate the \"background\" integrated quantity of a given variable\n",
    "    # First duplicate the original arrays\n",
    "    bbp_refprofs = bbp_bin3.copy(deep=True) \n",
    "    chl_refprofs = chl_bin3.copy(deep=True)\n",
    "    poc_refprofs = poc_bin3.copy(deep=True)\n",
    "    chl_bbp_refprofs = chl_bbp_ratio3.copy(deep=True)\n",
    "    aou_refprofs = aou_bin3.copy(deep=True)\n",
    "    spice_refprofs = spicebin3.copy(deep=True)\n",
    "    \n",
    "    # Then make them all NaN\n",
    "    bbp_refprofs[:,:], chl_refprofs[:,:], poc_refprofs[:,:], chl_bbp_refprofs[:,:], aou_refprofs[:,:], spice_refprofs[:,:] = np.nan,np.nan,np.nan,np.nan,np.nan,np.nan\n",
    "    \n",
    "    ###############\n",
    "    \n",
    "    \n",
    "    for i in range(len(db.N_PROF)): # For each profile:   \n",
    "        \n",
    "        prof_anoms = [] # Record the values of AOU anomalies in this vertical profile, as well as their depth indices [[magnitude,index]]\n",
    "        \n",
    "        if np.any(np.where(depths[i]<600)) and S[i].dropna(dim='N_LEVELS')[-1]<35 and -30>db.Lat[i]>-65 and ~np.isnan(db.Mixed_Layer_Depth[i]) and (floatid,i) not in itertools.chain(bad_t,bad_s,bad_oxygen):\n",
    "            index600 = np.where(depths[i]<600)[0][0] # Get the first index of profie i where depth < 600 m (depth array = [deep:shallow])\n",
    "            spice600 = spicebin3[i,index600]\n",
    "            spicesurface = spicebin3[i,int(indicesMLD[i]):].median()\n",
    "               \n",
    "        if np.any(np.where(depths[i]<600)) and S[i].dropna(dim='N_LEVELS')[-1]<35 and -30>db.Lat[i]>-65 and ~np.isnan(db.Mixed_Layer_Depth[i]) and (floatid,i) not in itertools.chain(bad_t,bad_s,bad_oxygen) and spicesurface<spice600: # If the profile reached a depth shallower than 600 m, and has a surface Salinity <35 (to discard tropical water incursions), is between -30 and -65 latitude, and had a valid MLD calculation (not faulty CTD), and had lower surface spice than at 600m (SO waters)\n",
    "            \n",
    "            lons_all.append(db.Lon[i].item()) # Record the profile as considered (coordinates)\n",
    "            lats_all.append(db.Lat[i].item())\n",
    "            T_all.extend(T_bin3[i].data)\n",
    "            S_all.extend(S_bin3[i].data)\n",
    "            depth_all.extend(depth_bin3[i].data)\n",
    "            floatid_all.extend(np.full(np.shape(T_bin3[i]),floatid))\n",
    "            station_all.extend(np.full(np.shape(T_bin3[i]),db.Station[i].data))\n",
    "            \n",
    "            index600 = np.where(depths[i]<600)[0][0] # Get the first index of profie i where depth < 600 m (depth array = [deep:shallow])\n",
    "            index1000 = np.where(depths[i]<1000)[0][0] # Get the first index of profie i where depth < 1000 m (depth array = [deep:shallow]) \n",
    "            \n",
    "            if np.any(np.where(depths[i]>100)): # Get the last index where depth > 100 m\n",
    "                index100 = np.where(depths[i]>100)[0][-1]\n",
    "            else:\n",
    "                index100 = np.nan\n",
    "                \n",
    "            upperbound = int(np.nanmin([index100,indicesMLD[i]])) # Stop either 100 m or the MLD, whichever comes first\n",
    " \n",
    "            # Calculate some BGC properties of the surface mixed layer, and append them to the dataframe\n",
    "            try:\n",
    "                SurfIntBbp = -bbp_bin3[i,int(indicesMLD[i]):].dropna(\"N_LEVELS\").integrate('Depth').item() # Integrate from the mixed layer to the surface\n",
    "                SurfIntChl = -chl_bin3[i,int(indicesMLD[i]):].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                SurfIntPOC = -poc_bin3[i,int(indicesMLD[i]):].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                SurfIntChlBbp = -chl_bbp_ratio3[i,int(indicesMLD[i]):].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                SurfIntT = -T_bin3[i,int(indicesMLD[i]):].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                SurfIntS = -S_bin3[i,int(indicesMLD[i]):].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                SurfIntSigma = -sigmagsw_bin3[i,int(indicesMLD[i]):].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                SurfIntSpice = -spicebin3[i,int(indicesMLD[i]):].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                SurfIntAOU = -aou_bin3[i,int(indicesMLD[i]):].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                \n",
    "                    \n",
    "            except (IndexError,ValueError) as error:\n",
    "                SurfIntBbp = np.nan\n",
    "                SurfIntChl = np.nan\n",
    "                SurfIntPOC = np.nan\n",
    "                SurfIntChlBbp = np.nan\n",
    "                SurfIntT = np.nan\n",
    "                SurfIntS = np.nan\n",
    "                SurfIntSigma = np.nan\n",
    "                SurfIntSpice = np.nan\n",
    "                SurfIntAOU = np.nan\n",
    "                \n",
    "            # SPIKE ANALYSIS: For navis floats: also calculate how many bbp and chl spikes there are between the MLD and 600m\n",
    "            if floatid in navis_floats and index600 < indicesMLD[i]: \n",
    "                # Calculate the number of spikes in the mesopelagic that meet thresholds:\n",
    "                # bbp: between 0.001 and 0.004 m^-1 (Rembeauville et al 2017: larger spikes are likely motile organisms, not large particles)\n",
    "                # chl: > 0.2 mg m^-3\n",
    "                \n",
    "                index600_navis = np.where(db.Depth[i]<600)[0][0] # Get the first index of profie i where depth < 600 m (depth array = [deep:shallow])\n",
    "                index1000_navis = np.where(db.Depth[i]<1000)[0][0] # Get the first index of profie i where depth < 1000 m (depth array = [deep:shallow]) \n",
    "            \n",
    "                if np.any(np.where(db.Depth[i]>100)): # Get the last index where depth > 100 m\n",
    "                    index100_navis = np.where(db.Depth[i]>100)[0][-1]\n",
    "                else:\n",
    "                    index100_navis = np.nan\n",
    "                \n",
    "                num_bbp_spikes = len(np.where((bbp_spikes[i,index600_navis:int(indicesMLD_navis[i])+1]>0.001) & (bbp_spikes[i,index600_navis:int(indicesMLD_navis[i])+1]<0.004))[0]) # The number of qualifying spikes\n",
    "                num_bbp_obs = len(bbp_spikes[i,index600_navis:int(indicesMLD_navis[i])+1]) # total number of mesopelagic observations\n",
    "                perc_bbp_spikes = (num_bbp_spikes/num_bbp_obs)*100 # spikes, normalized to observations\n",
    "                \n",
    "                num_chl_spikes = len(np.where(chl_spikes[i,index600_navis:int(indicesMLD_navis[i])+1]>0.2)[0]) # The number of qualifying spikes\n",
    "                num_chl_obs = len(chl_spikes[i,index600_navis:int(indicesMLD_navis[i])+1]) # total number of mesopelagic observations\n",
    "                perc_chl_spikes = (num_chl_spikes/num_chl_obs)*100 # spikes, normalized to observations\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                num_bbp_spikes =  np.nan\n",
    "                num_bbp_obs =  np.nan\n",
    "                perc_bbp_spikes = np.nan\n",
    "\n",
    "                num_chl_spikes =  np.nan\n",
    "                num_chl_obs =  np.nan\n",
    "                perc_chl_spikes = np.nan\n",
    "            \n",
    "            ### Integrate the total quantity of POC below the mixed layer\n",
    "            try:\n",
    "                DeepIntBbp = -bbp_bin3[i,index1000:int(indicesMLD[i])].dropna(\"N_LEVELS\").integrate('Depth').item() # Integrate from ~1000m depth to the mixed layer\n",
    "                DeepIntChl = -chl_bin3[i,index1000:int(indicesMLD[i])].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                DeepIntPOC = -poc_bin3[i,index1000:int(indicesMLD[i])].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                DeepIntChlBbp = -chl_bbp_ratio3[i,index1000:int(indicesMLD[i])].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                DeepIntT = -T_bin3[i,index1000:int(indicesMLD[i])].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                DeepIntS = -S_bin3[i,index1000:int(indicesMLD[i])].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                DeepIntSigma = -sigmagsw_bin3[i,index1000:int(indicesMLD[i])].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                DeepIntSpice = -spicebin3[i,index1000:int(indicesMLD[i])].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                DeepIntAOU = -aou_bin3[i,index1000:int(indicesMLD[i])].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                \n",
    "                    \n",
    "            except (IndexError,ValueError) as error:\n",
    "                DeepIntBbp = np.nan\n",
    "                DeepIntChl = np.nan\n",
    "                DeepIntPOC = np.nan\n",
    "                DeepIntChlBbp = np.nan\n",
    "                DeepIntT = np.nan\n",
    "                DeepIntS = np.nan\n",
    "                DeepIntSigma = np.nan\n",
    "                DeepIntSpice = np.nan\n",
    "                DeepIntAOU = np.nan\n",
    "\n",
    "            surface_bgc.append([floatid,db.Station[i].item(),db.Lon[i].item(),db.Lat[i].item(),db.mon_day_yr[i].item().decode('utf-8'),\\\n",
    "                                SurfIntBbp,SurfIntChl,SurfIntPOC,SurfIntChlBbp,SurfIntT,SurfIntS,SurfIntSigma,SurfIntSpice,SurfIntAOU,\\\n",
    "                                DeepIntBbp,DeepIntChl,DeepIntPOC,DeepIntChlBbp,DeepIntT,DeepIntS,DeepIntSigma,DeepIntSpice,DeepIntAOU,\\\n",
    "                                MLD[i],maxn2[i],perc_bbp_spikes,perc_chl_spikes])\n",
    "            \n",
    "            if index600 < indicesMLD[i]: # If the mixed layer is shallower than 600 m (otherwise ignore the profile)\n",
    "                \n",
    "                # Find local maxima in 3-bin smoothed profiles using scipy's find_peaks method: https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html\n",
    "                aou_peaks = find_peaks(-aou_bin3[i])[0] # Returns the indices of the peaks. Negative input in order to detect minima\n",
    "                spice_peaks = find_peaks(-spicebin3[i])[0]\n",
    "                #poc_peaks = find_peaks(poc_bin3[i])[0] # Detect positive POC peaks\n",
    "                \n",
    "                if np.any(aou_peaks) and np.any(spice_peaks): # If there are peaks in both AOU and spice profiles\n",
    "                    \n",
    "                    # Check that there are peaks between index600 and upperbound+1\n",
    "                    aou_peaks = aou_peaks[np.where((aou_peaks<upperbound) & (aou_peaks>index600))[0]] # Filter peak indices -- only those indices in the given depth range\n",
    "                    spice_peaks = spice_peaks[np.where((spice_peaks<upperbound) & (spice_peaks>index600))[0]]\n",
    "                    \n",
    "                    if np.any(aou_peaks) and np.any(spice_peaks): # If both AOU and spice have peaks in the given depth range\n",
    "                    \n",
    "                        for x in aou_peaks: # Pairwise assessment of each AOU and spice peak pair\n",
    "                            for y in spice_peaks: # Note: define AOU and spice at their *respective* peak depths (may be slightly offset). MAY REVISIT LATER?\n",
    "                                \n",
    "                                if (abs(aou_bin3[i].Depth[x] - spicebin3[i].Depth[y]) < 30): # The peaks must be be within 30 m of each other (for most floats, this is 3 sampling intervals)\n",
    "                                    \n",
    "                                    # Record the depths and values of the AOU and spice peaks\n",
    "                                    aou_peakdepth = aou_bin3[i,x].Depth.item()\n",
    "                                    spice_peakdepth = spicebin3[i,y].Depth.item()\n",
    "                                    \n",
    "                                    aou_peakval  = aou_bin3[i,x].item()\n",
    "                                    spice_peakval = spicebin3[i,y].item()\n",
    "                                    \n",
    "                                    ####### Define the reference profiles (without subduction anomalies)\n",
    "                                    \n",
    "                                    ### 1) Start by identifying the points 100 m above and below each peak\n",
    "                                    aou_reference_lower_i = np.where(depths[i]<aou_peakdepth+100)[0][0] # Index of the point 100 m below the AOU peak (less than/equal to)\n",
    "                                    aou_reference_upper_i = np.where(depths[i]>aou_peakdepth-100)[0][-1] # Index of the point 100 m above the AOU peak (less than/equal to)\n",
    "                                    \n",
    "                                    spice_reference_lower_i = np.where(depths[i]<spice_peakdepth+100)[0][0] #  Index of the point 100 m below the spice peak (less than/equal to)\n",
    "                                    spice_reference_upper_i = np.where(depths[i]>spice_peakdepth-100)[0][-1] ## Index of the point 100 m above the spice peak (less than/equal to)\n",
    "                                    \n",
    "                                    ### 2) For each depth interval (100 m above, 100m below), identify the maximum (most positive) AOU or spice value\n",
    "                                    aou_upperbound_maxval = aou_bin3[i, x:aou_reference_upper_i+1].max() # Value of the maximum AOU value in the (peak+100m) depth range (may have duplicates)\n",
    "                                    aou_upperbound_max_i = np.where(aou_bin3[i, x:aou_reference_upper_i+1]==aou_upperbound_maxval)[0][0] # Find the first index in this range (deepest depth) where this maximum value occurs\n",
    "                                    aou_upperbound_max = aou_bin3[i, x:aou_reference_upper_i+1][aou_upperbound_max_i].item() # The maximum value and its depth\n",
    "                                    aou_upperbound_max_depth = aou_bin3[i, x:aou_reference_upper_i+1][aou_upperbound_max_i].Depth.item() # The maximum value and its depth\n",
    "                                    \n",
    "                                    aou_lowerbound_maxval = aou_bin3[i,aou_reference_lower_i:x].max() # Value of the maximum AOU value in the (peak+100m) depth range (may have duplicates)\n",
    "                                    aou_lowerbound_max_i = np.where(aou_bin3[i,aou_reference_lower_i:x]==aou_lowerbound_maxval)[0][-1] # Find the last index in this range (shallowest depth) where this maximum value occurs\n",
    "                                    aou_lowerbound_max = aou_bin3[i,aou_reference_lower_i:x][aou_lowerbound_max_i].item() # The maximum value and its depth\n",
    "                                    aou_lowerbound_max_depth = aou_bin3[i,aou_reference_lower_i:x][aou_lowerbound_max_i].Depth.item() # The maximum value and its depth\n",
    "                                    \n",
    "                                    spice_upperbound_maxval = spicebin3[i, y:spice_reference_upper_i+1].max() # Value of the maximum spice value in the (peak+100m) depth range (may have duplicates)\n",
    "                                    spice_upperbound_max_i = np.where(spicebin3[i, y:spice_reference_upper_i+1]==spice_upperbound_maxval)[0][0] # Find the first index in this range (deepest depth) where this maximum value occurs\n",
    "                                    spice_upperbound_max = spicebin3[i, y:spice_reference_upper_i+1][spice_upperbound_max_i].item() # The maximum value and its depth\n",
    "                                    spice_upperbound_max_depth = spicebin3[i, y:spice_reference_upper_i+1][spice_upperbound_max_i].Depth.item() # The maximum value and its depth\n",
    "                                    \n",
    "                                    spice_lowerbound_maxval = spicebin3[i,spice_reference_lower_i:y].max() # Value of the maximum spice value in the (peak+100m) depth range (may have duplicates)\n",
    "                                    spice_lowerbound_max_i = np.where(spicebin3[i,spice_reference_lower_i:y]==spice_lowerbound_maxval)[0][-1] # Find the last index in this range (shallowest depth) where this maximum value occurs\n",
    "                                    spice_lowerbound_max = spicebin3[i,spice_reference_lower_i:y][spice_lowerbound_max_i].item() # The maximum value and its depth\n",
    "                                    spice_lowerbound_max_depth = spicebin3[i,spice_reference_lower_i:y][spice_lowerbound_max_i].Depth.item() # The maximum value and its depth\n",
    "                                    \n",
    "                                    ### 3) Construct the reference profile: draw a straight line between the maximum values above and below the peak\n",
    "                                    aou_reference_x = np.array([aou_upperbound_max_depth, aou_lowerbound_max_depth]).reshape((-1, 1)) # Independent variable: depth\n",
    "                                    aou_reference_y = np.array([aou_upperbound_max, aou_lowerbound_max]) # Dependent variable: predicted AOU value\n",
    "                                    \n",
    "                                    aou_model = LinearRegression().fit(aou_reference_x, aou_reference_y) # Construct the regression model \n",
    "                                    \n",
    "                                    spice_reference_x = np.array([spice_upperbound_max_depth, spice_lowerbound_max_depth]).reshape((-1, 1)) # Independent variable: depth\n",
    "                                    spice_reference_y = np.array([spice_upperbound_max, spice_lowerbound_max]) # Dependent variable: predicted spice value\n",
    "                                    \n",
    "                                    spice_model = LinearRegression().fit(spice_reference_x, spice_reference_y) # Construct the regression model \n",
    "                                    \n",
    "\n",
    "                                    ### 3b) Iteratively adjust the reference profile so that it doesn't intersect the observed profile\n",
    "                                    # Construct the reference profile at all depths\n",
    "                                    aou_refprof_iter_lower_i = np.where(aou_bin3[i].Depth==aou_lowerbound_max_depth)[0][0]\n",
    "                                    aou_refprof_iter_upper_i = np.where(aou_bin3[i].Depth==aou_upperbound_max_depth)[0][0]\n",
    "                                    aou_refprof_iter_depths = aou_bin3[i,aou_refprof_iter_lower_i:aou_refprof_iter_upper_i+1].Depth.values\n",
    "                                    aou_refprof_iter_depths_input = aou_refprof_iter_depths.reshape((-1,1))\n",
    "                                    aou_refprof_iter_vals = aou_model.predict(aou_refprof_iter_depths_input)\n",
    "                                    aou_refprof_iter_observed = aou_bin3[i,aou_refprof_iter_lower_i:aou_refprof_iter_upper_i+1].values\n",
    "                                \n",
    "                                    aou_refprof_iter_peakindex = np.where(aou_refprof_iter_depths==aou_peakdepth)[0][0] # The index within the reference profile of the peak\n",
    "                                    aou_refprof_iter_diff = aou_refprof_iter_vals - aou_refprof_iter_observed\n",
    "                                    \n",
    "                                    if np.any(aou_refprof_iter_diff[0:aou_refprof_iter_peakindex] < -1e-13): # If the bottom of the observed profile goes above (more + than) the reference profile\n",
    "                                        intersectLower = True\n",
    "                                        \n",
    "                                        while (aou_refprof_iter_lower_i < x-1) and intersectLower==True: \n",
    "                                            if np.isnan(aou_bin3[i,aou_refprof_iter_lower_i+1]): #If the next index is an nan, stop the optimization routine\n",
    "                                                break\n",
    "                                                \n",
    "                                            aou_refprof_iter_lower_i +=1 # Iterate the lowerbound one index shallower at a time\n",
    "                                            \n",
    "                                            aou_refprof_iter_depths = aou_bin3[i,aou_refprof_iter_lower_i:aou_refprof_iter_upper_i+1].Depth.values\n",
    "                                            aou_refprof_iter_depths_input = aou_refprof_iter_depths.reshape((-1,1))\n",
    "                                            aou_refprof_iter_observed = aou_bin3[i,aou_refprof_iter_lower_i:aou_refprof_iter_upper_i+1].values\n",
    "                                            aou_refprof_iter_peakindex = np.where(aou_refprof_iter_depths==aou_peakdepth)[0][0] # The index within the reference profile of the peak\n",
    "                                            \n",
    "                                            # Reconstruct the regression reference model\n",
    "                                            aou_reference_x = np.array([aou_refprof_iter_depths[-1], aou_refprof_iter_depths[0]]).reshape((-1, 1))\n",
    "                                            aou_reference_y = np.array([aou_refprof_iter_observed[-1], aou_refprof_iter_observed[0]])\n",
    "                                            aou_model = LinearRegression().fit(aou_reference_x, aou_reference_y)\n",
    "                                            \n",
    "                                            # Use the model to predict the reference profile again\n",
    "                                            aou_refprof_iter_vals = aou_model.predict(aou_refprof_iter_depths_input)\n",
    "                                            aou_refprof_iter_diff = aou_refprof_iter_vals - aou_refprof_iter_observed\n",
    "                                            \n",
    "                                            if np.any(aou_refprof_iter_diff[0:aou_refprof_iter_peakindex] < -1e-13) == False:\n",
    "                                                intersectLower = False\n",
    "                                    \n",
    "                                    if np.any(aou_refprof_iter_diff[aou_refprof_iter_peakindex:] < -1e-13): # If the top of the observed profile goes above (more + than) the reference profile\n",
    "                                        intersectUpper = True\n",
    "                                        \n",
    "                                        while (aou_refprof_iter_upper_i > x+1) and intersectUpper==True:\n",
    "                                            if np.isnan(aou_bin3[i,aou_refprof_iter_upper_i-1]):\n",
    "                                                break\n",
    "                                                \n",
    "                                            aou_refprof_iter_upper_i -=1 # Iterate the lowerbound one index shallower at a time\n",
    "                                            \n",
    "                                            aou_refprof_iter_depths = aou_bin3[i,aou_refprof_iter_lower_i:aou_refprof_iter_upper_i+1].Depth.values\n",
    "                                            aou_refprof_iter_depths_input = aou_refprof_iter_depths.reshape((-1,1))\n",
    "                                            aou_refprof_iter_observed = aou_bin3[i,aou_refprof_iter_lower_i:aou_refprof_iter_upper_i+1].values\n",
    "                                            aou_refprof_iter_peakindex = np.where(aou_refprof_iter_depths==aou_peakdepth)[0][0] # The index within the reference profile of the peak\n",
    "                                            \n",
    "                                            # Reconstruct the regression reference model\n",
    "                                            aou_reference_x = np.array([aou_refprof_iter_depths[-1], aou_refprof_iter_depths[0]]).reshape((-1, 1))\n",
    "                                            aou_reference_y = np.array([aou_refprof_iter_observed[-1], aou_refprof_iter_observed[0]])\n",
    "                                            aou_model = LinearRegression().fit(aou_reference_x, aou_reference_y)\n",
    "                                            \n",
    "                                            # Use the model to predict the reference profile again\n",
    "                                            aou_refprof_iter_vals = aou_model.predict(aou_refprof_iter_depths_input)\n",
    "                                            aou_refprof_iter_diff = aou_refprof_iter_vals - aou_refprof_iter_observed\n",
    "                                            \n",
    "                                            if np.any(aou_refprof_iter_diff[aou_refprof_iter_peakindex:] < -1e-13) == False:\n",
    "                                                intersectUpper = False\n",
    "                                    \n",
    "                                    ### 3c) Repeat for spice profile. Iteratively adjust the reference profile so that it doesn't intersect the observed profile\n",
    "                                    # Construct the reference profile at all depths\n",
    "                                    spice_refprof_iter_lower_i = np.where(spicebin3[i].Depth==spice_lowerbound_max_depth)[0][0]\n",
    "                                    spice_refprof_iter_upper_i = np.where(spicebin3[i].Depth==spice_upperbound_max_depth)[0][0]\n",
    "                                    spice_refprof_iter_depths = spicebin3[i,spice_refprof_iter_lower_i:spice_refprof_iter_upper_i+1].Depth.values\n",
    "                                    spice_refprof_iter_depths_input = spice_refprof_iter_depths.reshape((-1,1))\n",
    "                                    spice_refprof_iter_vals = spice_model.predict(spice_refprof_iter_depths_input)\n",
    "                                    spice_refprof_iter_observed = spicebin3[i,spice_refprof_iter_lower_i:spice_refprof_iter_upper_i+1].values\n",
    "                                    \n",
    "                                    spice_refprof_iter_peakindex = np.where(spice_refprof_iter_depths==spice_peakdepth)[0][0] # The index within the reference profile of the peak\n",
    "                                    spice_refprof_iter_diff = spice_refprof_iter_vals - spice_refprof_iter_observed\n",
    "                                    \n",
    "                                    if np.any(spice_refprof_iter_diff[0:spice_refprof_iter_peakindex] < -1e-15): # If the bottom of the observed profile goes above (more + than) the reference profile\n",
    "                                        intersectLower = True\n",
    "                                        \n",
    "                                        while (spice_refprof_iter_lower_i < y-1) and intersectLower==True:\n",
    "                                            if np.isnan(spicebin3[i,spice_refprof_iter_lower_i+1]):\n",
    "                                                break\n",
    "                                                \n",
    "                                            spice_refprof_iter_lower_i +=1 # Iterate the lowerbound one index shallower at a time\n",
    "                                            \n",
    "                                            spice_refprof_iter_depths = spicebin3[i,spice_refprof_iter_lower_i:spice_refprof_iter_upper_i+1].Depth.values\n",
    "                                            spice_refprof_iter_depths_input = spice_refprof_iter_depths.reshape((-1,1))\n",
    "                                            spice_refprof_iter_observed = spicebin3[i,spice_refprof_iter_lower_i:spice_refprof_iter_upper_i+1].values\n",
    "                                            spice_refprof_iter_peakindex = np.where(spice_refprof_iter_depths==spice_peakdepth)[0][0] # The index within the reference profile of the peak\n",
    "                                            \n",
    "                                            spice_reference_x = np.array([spice_refprof_iter_depths[-1], spice_refprof_iter_depths[0]]).reshape((-1, 1))\n",
    "                                            spice_reference_y = np.array([spice_refprof_iter_observed[-1], spice_refprof_iter_observed[0]])\n",
    "                                            spice_model = LinearRegression().fit(spice_reference_x, spice_reference_y)\n",
    "                                            \n",
    "                                            # Use the model to predict the reference profile again\n",
    "                                            spice_refprof_iter_vals = spice_model.predict(spice_refprof_iter_depths_input)\n",
    "                                            spice_refprof_iter_diff = spice_refprof_iter_vals - spice_refprof_iter_observed\n",
    "                                            \n",
    "                                            if np.any(spice_refprof_iter_diff[0:spice_refprof_iter_peakindex] < -1e-15) == False:\n",
    "                                                intersectLower = False\n",
    "                                    \n",
    "                                    if np.any(spice_refprof_iter_diff[spice_refprof_iter_peakindex:] < -1e-15): # If the top of the observed profile goes above (more + than) the reference profile\n",
    "                                        intersectUpper = True\n",
    "                                        \n",
    "                                        while (spice_refprof_iter_upper_i > y+1) and intersectUpper==True:\n",
    "                                            if np.isnan(spicebin3[i,spice_refprof_iter_upper_i-1]):\n",
    "                                                break\n",
    "                                                \n",
    "                                            spice_refprof_iter_upper_i -=1 # Iterate the lowerbound one index shallower at a time\n",
    "                                            \n",
    "                                            spice_refprof_iter_depths = spicebin3[i,spice_refprof_iter_lower_i:spice_refprof_iter_upper_i+1].Depth.values\n",
    "                                            spice_refprof_iter_depths_input = spice_refprof_iter_depths.reshape((-1,1))\n",
    "                                            spice_refprof_iter_observed = spicebin3[i,spice_refprof_iter_lower_i:spice_refprof_iter_upper_i+1].values\n",
    "                                            spice_refprof_iter_peakindex = np.where(spice_refprof_iter_depths==spice_peakdepth)[0][0] # The index within the reference profile of the peak\n",
    "                                                \n",
    "                                            spice_reference_x = np.array([spice_refprof_iter_depths[-1], spice_refprof_iter_depths[0]]).reshape((-1, 1))\n",
    "                                            spice_reference_y = np.array([spice_refprof_iter_observed[-1], spice_refprof_iter_observed[0]])\n",
    "                                            spice_model = LinearRegression().fit(spice_reference_x, spice_reference_y)\n",
    "                                            \n",
    "                                            # Use the model to predict the reference profile again\n",
    "                                            spice_refprof_iter_vals = spice_model.predict(spice_refprof_iter_depths_input)\n",
    "                                            spice_refprof_iter_diff = spice_refprof_iter_vals - spice_refprof_iter_observed\n",
    "                                            \n",
    "                                            if np.any(spice_refprof_iter_diff[spice_refprof_iter_peakindex:] < -1e-15) == False:\n",
    "                                                intersectUpper = False\n",
    "                                    \n",
    "                                    \n",
    "                                    ### 4) Determine the anomaly size at the peak\n",
    "                                    aou_refval = aou_model.predict([[aou_peakdepth]]).item()\n",
    "                                    spice_refval = spice_model.predict([[spice_peakdepth]]).item()\n",
    "                                    \n",
    "                                    ######## Determine if it's a true anomaly. Must meet 4 conditions to be an anomaly\n",
    "                                    # Depth of the AOU and spice peaks must be within 30 m of each other (for most floats, this is 3 sampling intervals)\n",
    "                                    # The AOU anomaly (diff between 3-bin and 20-bin medians) must be < -8\n",
    "                                    # The spice anomaly (diff between 3-bin and 20-bin medians) must be < -0.05\n",
    "                                    # Must be at least 100 m below the MLD\n",
    "                                    \n",
    "                                    if np.less(aou_peakval - aou_refval, -8) and np.less(spice_peakval - spice_refval, -0.05) and aou_peakdepth > (MLD[i]+100):\n",
    "                                        # Record this profile's anomalies: [AOU N_LEVELs index, AOU magnitude, spice N_LEVELS index, spice magnitude, AOU reference values (upper/lower), spice reference values (upper/lower), predicted AOU reference, predicted spice reference]\n",
    "                                        #prof_anoms.append([x, aou_peakval - aou_refval, y, spice_peakval - spice_refval, aou_reference_x.reshape(1,-1)[0], aou_reference_y, spice_reference_x.reshape(1,-1)[0], spice_reference_y, aou_refval, spice_refval]) \n",
    "                                        prof_anoms.append([x, aou_peakval, aou_peakval - aou_refval, y, spice_peakval, spice_peakval - spice_refval, aou_refprof_iter_depths[-1], aou_refprof_iter_upper_i, aou_refprof_iter_depths[0], aou_refprof_iter_lower_i, aou_refprof_iter_observed[-1], aou_refprof_iter_observed[0], spice_refprof_iter_depths[-1], spice_refprof_iter_depths[0], spice_refprof_iter_observed[-1], spice_refprof_iter_observed[0], aou_refval, spice_refval]) \n",
    "                                    \n",
    "                    if np.any(prof_anoms): # If the profile had any spice/AOU anomalies\n",
    "                        \n",
    "                        #prof_anoms = pd.DataFrame(prof_anoms, columns=['AOU_N_LEVEL', 'AOU_magnitude','Spice_N_LEVEL', 'Spice_magnitude', 'AOU_ref_x', 'AOU_ref_y', 'spice_ref_x','spice_ref_y','AOU_refval_pred','spice_refval_pred']) # Turn the anomalies into a dataframe to allow manipulation\n",
    "                        prof_anoms = pd.DataFrame(prof_anoms, columns=['AOU_N_LEVEL', 'AOU_value', 'AOU_magnitude','Spice_N_LEVEL', 'Spice_value', 'Spice_magnitude','AOU_ref_upper_depth', 'AOU_ref_upper_index', 'AOU_ref_lower_depth', 'AOU_ref_lower_index', 'AOU_ref_upper','AOU_ref_lower','spice_ref_upper_depth','spice_ref_lower_depth','spice_ref_upper','spice_ref_lower','AOU_refval_pred','spice_refval_pred']) # Turn the anomalies into a dataframe to allow manipulation\n",
    "                        \n",
    "                        for aou_i in prof_anoms.AOU_N_LEVEL.unique(): # For each unique AOU anomaly peak in this profile (if multiple)\n",
    "                            aou_i = int(aou_i) #convert float to integer if necessary, to allow indexing \n",
    "                            \n",
    "                            #If there are multiple spice peaks associated with this AOU peak, pick only the largest anomaly to append\n",
    "                            toAppend = prof_anoms[prof_anoms['AOU_N_LEVEL']==aou_i].nsmallest(1, 'Spice_magnitude') #filter dataframe for this spice peak\n",
    "                            spice_i = int(toAppend.Spice_N_LEVEL.unique().item()) # get the spice peak's index\n",
    "                            \n",
    "                            # Calculate integrated values of BGC values within the anomaly. For all values, integrate through the upper/lower bounds of the AOU anomaly\n",
    "                            # a) Fill in the \"reference\" values of the variables at the top/bottom of the anomaly, in the empty data variables created above\n",
    "                            bbp_refprofs[i,toAppend.AOU_ref_upper_index.item()] = bbp_bin3[i,toAppend.AOU_ref_upper_index.item()].item()\n",
    "                            bbp_refprofs[i,toAppend.AOU_ref_lower_index.item()] = bbp_bin3[i,toAppend.AOU_ref_lower_index.item()].item()\n",
    "                            \n",
    "                            chl_refprofs[i,toAppend.AOU_ref_upper_index.item()] = chl_bin3[i,toAppend.AOU_ref_upper_index.item()].item()\n",
    "                            chl_refprofs[i,toAppend.AOU_ref_lower_index.item()] = chl_bin3[i,toAppend.AOU_ref_lower_index.item()].item()\n",
    "                            \n",
    "                            poc_refprofs[i,toAppend.AOU_ref_upper_index.item()] = poc_bin3[i,toAppend.AOU_ref_upper_index.item()].item()\n",
    "                            poc_refprofs[i,toAppend.AOU_ref_lower_index.item()] = poc_bin3[i,toAppend.AOU_ref_lower_index.item()].item()\n",
    "                            \n",
    "                            chl_bbp_refprofs[i,toAppend.AOU_ref_upper_index.item()] = chl_bbp_ratio3[i,toAppend.AOU_ref_upper_index.item()].item()\n",
    "                            chl_bbp_refprofs[i,toAppend.AOU_ref_lower_index.item()] = chl_bbp_ratio3[i,toAppend.AOU_ref_lower_index.item()].item()\n",
    "                            \n",
    "                            aou_refprofs[i,toAppend.AOU_ref_upper_index.item()] = aou_bin3[i,toAppend.AOU_ref_upper_index.item()].item()\n",
    "                            aou_refprofs[i,toAppend.AOU_ref_lower_index.item()] = aou_bin3[i,toAppend.AOU_ref_lower_index.item()].item()\n",
    "                            \n",
    "                            spice_refprofs[i,toAppend.AOU_ref_upper_index.item()] = spicebin3[i,toAppend.AOU_ref_upper_index.item()].item()\n",
    "                            spice_refprofs[i,toAppend.AOU_ref_lower_index.item()] = spicebin3[i,toAppend.AOU_ref_lower_index.item()].item()\n",
    "                            \n",
    "                            # b) Integrate each variable through the depth range of the anomaly. Subtract the integrated value of the \"reference\" profile. Integrated values are negative. Flip the signs\n",
    "                            \n",
    "                            try:\n",
    "                                # Total = variables integrated through the thickness of the anomaly\n",
    "                                tot_IntBbp = -bbp_bin3[i,toAppend.AOU_ref_lower_index.item():toAppend.AOU_ref_upper_index.item()+1].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                                tot_IntChl = -chl_bin3[i,toAppend.AOU_ref_lower_index.item():toAppend.AOU_ref_upper_index.item()+1].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                                tot_IntPOC = -poc_bin3[i,toAppend.AOU_ref_lower_index.item():toAppend.AOU_ref_upper_index.item()+1].dropna(\"N_LEVELS\").integrate('Depth').item() \n",
    "                                tot_IntChlBbp = -chl_bbp_ratio3[i,toAppend.AOU_ref_lower_index.item():toAppend.AOU_ref_upper_index.item()+1].dropna(\"N_LEVELS\").integrate('Depth').item() \n",
    "                                tot_IntAOU = -aou_bin3[i,toAppend.AOU_ref_lower_index.item():toAppend.AOU_ref_upper_index.item()+1].dropna(\"N_LEVELS\").integrate('Depth').item() \n",
    "                                tot_IntSpice = -spicebin3[i,toAppend.AOU_ref_lower_index.item():toAppend.AOU_ref_upper_index.item()+1].dropna(\"N_LEVELS\").integrate('Depth').item() \n",
    "                                tot_IntT = -T_bin3[i,toAppend.AOU_ref_lower_index.item():toAppend.AOU_ref_upper_index.item()+1].dropna(\"N_LEVELS\").integrate('Depth').item() \n",
    "                                tot_IntS = -S_bin3[i,toAppend.AOU_ref_lower_index.item():toAppend.AOU_ref_upper_index.item()+1].dropna(\"N_LEVELS\").integrate('Depth').item() \n",
    "                                tot_IntSigma = -sigmagsw_bin3[i,toAppend.AOU_ref_lower_index.item():toAppend.AOU_ref_upper_index.item()+1].dropna(\"N_LEVELS\").integrate('Depth').item() \n",
    "                                \n",
    "                                # Reference = variables integrated using a straight line between the (3-bin smoothed) values at the top and bottom of the anomaly\n",
    "                                ref_IntBbp = -bbp_refprofs[i,toAppend.AOU_ref_lower_index.item():toAppend.AOU_ref_upper_index.item()+1].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                                ref_IntChl = -chl_refprofs[i,toAppend.AOU_ref_lower_index.item():toAppend.AOU_ref_upper_index.item()+1].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                                ref_IntPOC = -poc_refprofs[i,toAppend.AOU_ref_lower_index.item():toAppend.AOU_ref_upper_index.item()+1].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                                ref_IntChlBbp = -chl_bbp_refprofs[i,toAppend.AOU_ref_lower_index.item():toAppend.AOU_ref_upper_index.item()+1].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                                ref_IntAOU = -aou_refprofs[i,toAppend.AOU_ref_lower_index.item():toAppend.AOU_ref_upper_index.item()+1].dropna(\"N_LEVELS\").integrate('Depth').item()\n",
    "                                ref_IntSpice = -spicebin3[i,toAppend.AOU_ref_lower_index.item():toAppend.AOU_ref_upper_index.item()+1].dropna(\"N_LEVELS\").integrate('Depth').item() \n",
    "                                \n",
    "                                # Anomaly value = (total - reference) integrated values, ie \"excess quantity due to subduction\"\n",
    "                                anom_IntBbp = tot_IntBbp - ref_IntBbp\n",
    "                                anom_IntChl = tot_IntChl - ref_IntChl\n",
    "                                anom_IntPOC = tot_IntPOC - ref_IntPOC\n",
    "                                anom_IntChlBbp = tot_IntChlBbp - ref_IntChlBbp\n",
    "                                anom_IntAOU = tot_IntAOU - ref_IntAOU\n",
    "                                anom_IntSpice = tot_IntSpice - ref_IntSpice\n",
    "                                \n",
    "                            except (IndexError,ValueError) as error: # nan if unable to integrate\n",
    "                                tot_IntBbp = np.nan\n",
    "                                tot_IntChl = np.nan\n",
    "                                tot_IntPOC = np.nan\n",
    "                                tot_IntChlBbp = np.nan\n",
    "                                tot_IntAOU = np.nan\n",
    "                                tot_IntSpice = np.nan\n",
    "                                tot_IntT = np.nan\n",
    "                                tot_IntS = np.nan\n",
    "                                tot_IntSigma = np.nan\n",
    "                                ref_IntBbp = np.nan\n",
    "                                ref_IntChl = np.nan\n",
    "                                ref_IntPOC = np.nan\n",
    "                                ref_IntChlBbp = np.nan\n",
    "                                ref_IntAOU = np.nan\n",
    "                                ref_IntSpice = np.nan\n",
    "                                anom_IntBbp = np.nan\n",
    "                                anom_IntChl = np.nan\n",
    "                                anom_IntPOC = np.nan\n",
    "                                anom_IntChlBbp = np.nan\n",
    "                                anom_IntAOU = np.nan\n",
    "                                anom_IntSpice = np.nan\n",
    "                                \n",
    "                            # Append data:\n",
    "                            #########   NOTE   ###########\n",
    "                            ########### Un-integrated BGC variables are recorded at depth of AOU anomaly\n",
    "                            \n",
    "                            anoms.append([floatid,db.Station[i].item(),db.Lon[i].item(),db.Lat[i].item(),db.mon_day_yr[i].item().decode('utf-8'),\\\n",
    "                                            toAppend.AOU_value.unique().item(), toAppend.AOU_magnitude.unique().item(), aou_bin3[i].Depth[aou_i].item(), toAppend.AOU_ref_upper_depth.item(),toAppend.AOU_ref_lower_depth.item(),toAppend.AOU_ref_upper.item(),toAppend.AOU_ref_lower.item(),toAppend.AOU_refval_pred.item(),\\\n",
    "                                          toAppend.Spice_value.unique().item(), toAppend.Spice_magnitude.unique().item(), spicebin3[i].Depth[spice_i].item(),toAppend.spice_ref_upper_depth.item(),toAppend.spice_ref_lower_depth.item(),toAppend.spice_ref_upper.item(),toAppend.spice_ref_lower.item(),toAppend.spice_refval_pred.unique().item(),\\\n",
    "                                            db.Mixed_Layer_Depth[i].item(),db.Max_N2[i].item(),poc_bin3[i,aou_i].item(),bbp_bin3[i,aou_i].item(),chl_bin3[i,aou_i].item(),sigmagsw_bin3[i,aou_i].item(), T_bin3[i,aou_i].item(), S_bin3[i,aou_i].item(), \\\n",
    "                                          tot_IntBbp, tot_IntChl, tot_IntPOC, tot_IntChlBbp, tot_IntAOU, tot_IntSpice, tot_IntT, tot_IntS, tot_IntSigma, \\\n",
    "                                          ref_IntBbp, ref_IntChl, ref_IntPOC, ref_IntChlBbp, ref_IntAOU, ref_IntSpice, \\\n",
    "                                          anom_IntBbp, anom_IntChl, anom_IntPOC, anom_IntChlBbp, anom_IntAOU, ref_IntSpice, \\\n",
    "                                          DeepIntBbp,DeepIntChl,DeepIntPOC,DeepIntChlBbp])\n",
    "     \n",
    "    \n",
    "# Turn anoms and surface_bgc into pandas dataframes with headers\n",
    "anoms = pd.DataFrame(anoms, columns=['Float','Station','Lon','Lat','mon_day_yr',\\\n",
    "                                     'AOU_value','AOU_anomaly','AOU_anomaly_depth','AOU_ref_upper_depth','AOU_ref_lower_depth','AOU_ref_upper','AOU_ref_lower','AOU_refval_pred',\\\n",
    "                                     'Spice_value','Spice_anomaly','Spice_anomaly_depth','spice_ref_upper_depth','spice_ref_lower_depth','spice_ref_upper','spice_ref_lower', 'spice_refval_pred',\\\n",
    "                                     'MLD','Max_N2','POC','b_bp700','Chl_a','Sigma','Conservative_Temperature','Absolute_Salinity',\\\n",
    "                                     'integrated_b_bp700_total', 'integrated_Chl_a_total', 'integrated_POC_total', 'chl_bbp_ratio_total', 'integrated_AOU_total','integrated_spice_total','integrated_T_total','integrated_S_total','integrated_sigma_total',\\\n",
    "                                     'integrated_b_bp700_ref', 'integrated_Chl_a_ref', 'integrated_POC_ref', 'chl_bbp_ratio_ref', 'integrated_AOU_ref', 'integrated_spice_ref',\\\n",
    "                                     'integrated_b_bp700_anom', 'integrated_Chl_a_anom', 'integrated_POC_anom', 'chl_bbp_ratio_anom', 'integrated_AOU_anom', 'integrated_spice_anom',\\\n",
    "                                     'Deep_integrated_b_bp700', 'Deep_integrated_Chl_a', 'Deep_integrated_POC', 'Deep_chl_bbp_ratio',])       \n",
    "\n",
    "\n",
    "surface_bgc = pd.DataFrame(surface_bgc, columns=['Float','Station','Lon','Lat','mon_day_yr',\\\n",
    "                                                 'MLD_integrated_b_bp700', 'MLD_integrated_Chl_a', 'MLD_integrated_POC', 'MLD_chl_bbp_ratio', 'MLD_integrated_T','MLD_integrated_S','MLD_integrated_sigma','MLD_integrated_spice','MLD_integrated_AOU',\\\n",
    "                                                 'Deep_integrated_b_bp700', 'Deep_integrated_Chl_a', 'Deep_integrated_POC', 'Deep_chl_bbp_ratio','Deep_integrated_T','Deep_integrated_S','Deep_integrated_sigma','Deep_integrated_spice','Deep_integrated_AOU', \\\n",
    "                                                 'MLD','Max_N2','mesopelagic_spike_percent_bbp','mesopelagic_spike_percent_chl'])\n",
    "surface_bgc['yearday'] = surface_bgc.mon_day_yr.apply(lambda x: datetime.strptime(x,\"%m/%d/%Y\").timetuple().tm_yday)\n",
    "surface_bgc['floatstation'] = surface_bgc['Float'].astype(str)+'_'+surface_bgc['Station'].astype(str)\n",
    "surface_bgc['mon_day_yr'] = pd.to_datetime(surface_bgc['mon_day_yr'])\n",
    "surface_bgc['month'] = pd.DatetimeIndex(surface_bgc.mon_day_yr).month\n",
    "surface_bgc['month_winter0'] = surface_bgc['month'].apply(lambda x: x+12 if x<6 else x)\n",
    "\n",
    "# Normalized values: average value in the mixed layer\n",
    "surface_bgc['MLD_integrated_b_bp700_norm'] = surface_bgc['MLD_integrated_b_bp700']/surface_bgc['MLD']\n",
    "surface_bgc['MLD_integrated_Chl_a_norm'] = surface_bgc['MLD_integrated_Chl_a']/surface_bgc['MLD']\n",
    "surface_bgc['MLD_integrated_POC_norm'] = surface_bgc['MLD_integrated_POC']/surface_bgc['MLD']\n",
    "surface_bgc['MLD_chl_bbp_ratio_norm'] = surface_bgc['MLD_chl_bbp_ratio']/surface_bgc['MLD']\n",
    "surface_bgc['MLD_integrated_T_norm'] = surface_bgc['MLD_integrated_T']/surface_bgc['MLD']\n",
    "surface_bgc['MLD_integrated_S_norm'] = surface_bgc['MLD_integrated_S']/surface_bgc['MLD']\n",
    "surface_bgc['MLD_integrated_sigma_norm'] = surface_bgc['MLD_integrated_sigma']/surface_bgc['MLD']\n",
    "surface_bgc['MLD_integrated_spice_norm'] = surface_bgc['MLD_integrated_spice']/surface_bgc['MLD']\n",
    "surface_bgc['MLD_integrated_AOU_norm'] = surface_bgc['MLD_integrated_AOU']/surface_bgc['MLD']\n",
    "\n",
    "# Normalized values: average values in the mesopelagic\n",
    "surface_bgc[\"depth_between_MLD_1000\"] = 1000 - surface_bgc['MLD']\n",
    "surface_bgc['Deep_integrated_b_bp700_norm'] = surface_bgc['Deep_integrated_b_bp700']/surface_bgc['depth_between_MLD_1000']\n",
    "surface_bgc['Deep_integrated_Chl_a'] = surface_bgc['Deep_integrated_Chl_a']/surface_bgc['depth_between_MLD_1000']\n",
    "surface_bgc['Deep_integrated_POC_norm'] = surface_bgc['Deep_integrated_POC']/surface_bgc['depth_between_MLD_1000']\n",
    "surface_bgc['Deep_chl_bbp_ratio_norm'] = surface_bgc['Deep_chl_bbp_ratio']/surface_bgc['depth_between_MLD_1000']\n",
    "surface_bgc['Deep_integrated_T_norm'] = surface_bgc['Deep_integrated_T']/surface_bgc['depth_between_MLD_1000']\n",
    "surface_bgc['Deep_integrated_S_norm'] = surface_bgc['Deep_integrated_S']/surface_bgc['depth_between_MLD_1000']\n",
    "surface_bgc['Deep_integrated_sigma_norm'] = surface_bgc['Deep_integrated_sigma']/surface_bgc['depth_between_MLD_1000']\n",
    "surface_bgc['Deep_integrated_spice_norm'] = surface_bgc['Deep_integrated_spice']/surface_bgc['depth_between_MLD_1000']\n",
    "surface_bgc['Deep_integrated_AOU_norm'] = surface_bgc['Deep_integrated_AOU']/surface_bgc['depth_between_MLD_1000']\n",
    "\n",
    "anoms['yearday'] = anoms.mon_day_yr.apply(lambda x: datetime.strptime(x,\"%m/%d/%Y\").timetuple().tm_yday)\n",
    "anoms['depth_below_mld'] = anoms['AOU_anomaly_depth']-anoms['MLD']\n",
    "anoms['floatstation'] = anoms['Float'].astype(str)+'_'+anoms['Station'].astype(str)\n",
    "anoms['floatstation_anomdepth'] = anoms['floatstation'] + '_' + anoms['AOU_anomaly_depth'].astype('int').astype('str') # each unique anomaly\n",
    "anoms['thickness'] = anoms['AOU_ref_lower_depth'] - anoms['AOU_ref_upper_depth']\n",
    "anoms['mon_day_yr'] = pd.to_datetime(anoms['mon_day_yr'])\n",
    "anoms['month'] = pd.DatetimeIndex(anoms.mon_day_yr).month\n",
    "anoms['month_winter0'] = anoms['month'].apply(lambda x: x+12 if x<6 else x)\n",
    "anoms['year'] = pd.DatetimeIndex(anoms.mon_day_yr).year\n",
    "\n",
    "# Normalized values: average value within the anomaly\n",
    "anoms['chl_bbp_ratio_total_norm'] = anoms['chl_bbp_ratio_total']/anoms['thickness']\n",
    "anoms['chl_bbp_ratio_ref_norm'] = anoms['chl_bbp_ratio_ref']/anoms['thickness']\n",
    "anoms['chl_bbp_ratio_anom_norm'] = anoms['chl_bbp_ratio_anom']/anoms['thickness']\n",
    "\n",
    "anoms['integrated_POC_total_norm'] = anoms['integrated_POC_total']/anoms['thickness']\n",
    "anoms['integrated_POC_ref_norm'] = anoms['integrated_POC_ref']/anoms['thickness']\n",
    "anoms['integrated_POC_anom_norm'] = anoms['integrated_POC_anom']/anoms['thickness']\n",
    "\n",
    "anoms['integrated_Chl_a_total_norm'] = anoms['integrated_Chl_a_total']/anoms['thickness']\n",
    "anoms['integrated_Chl_a_ref_norm'] = anoms['integrated_Chl_a_ref']/anoms['thickness']\n",
    "anoms['integrated_Chl_a_anom_norm'] = anoms['integrated_Chl_a_anom']/anoms['thickness']\n",
    "\n",
    "anoms['integrated_b_bp700_total_norm'] = anoms['integrated_b_bp700_total']/anoms['thickness']\n",
    "anoms['integrated_b_bp700_ref_norm'] = anoms['integrated_b_bp700_ref']/anoms['thickness']\n",
    "anoms['integrated_b_bp700_anom_norm'] = anoms['integrated_b_bp700_anom']/anoms['thickness']\n",
    "\n",
    "anoms['integrated_AOU_total_norm'] = anoms['integrated_AOU_total']/anoms['thickness']\n",
    "anoms['integrated_AOU_ref_norm'] = anoms['integrated_AOU_ref']/anoms['thickness']\n",
    "anoms['integrated_AOU_anom_norm'] = anoms['integrated_AOU_anom']/anoms['thickness']\n",
    "\n",
    "anoms['integrated_spice_total_norm'] = anoms['integrated_spice_total']/anoms['thickness']\n",
    "anoms['integrated_spice_ref_norm'] = anoms['integrated_spice_ref']/anoms['thickness']\n",
    "anoms['integrated_spice_anom_norm'] = anoms['integrated_spice_anom']/anoms['thickness']\n",
    "\n",
    "anoms['integrated_T_total_norm'] = anoms['integrated_T_total']/anoms['thickness']\n",
    "anoms['integrated_S_total_norm'] = anoms['integrated_S_total']/anoms['thickness']\n",
    "anoms['integrated_sigma_total_norm'] = anoms['integrated_sigma_total']/anoms['thickness']\n",
    "\n",
    "\n",
    "# Mark which profiles had a subduction anomaly detected\n",
    "surface_bgc['anomaly'] = surface_bgc.floatstation.apply(lambda x: 'y' if x in anoms['floatstation'].tolist() else 'n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a758937e-d19d-47dd-b618-81618a6f11f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ts_all = pd.DataFrame(np.transpose([T_all,S_all,depth_all,floatid_all,station_all]),columns=['Conservative_Temperature','Absolute_Salinity','Depth','Float','Station'])\n",
    "ts_all['floatstation'] = ts_all['Float'].astype(str)+'_'+ts_all['Station'].astype(str)\n",
    "ts_all['anomaly'] = ts_all.floatstation.apply(lambda x: 'y' if x in anoms['floatstation'].tolist() else 'n')\n",
    "\n",
    "ts_all[\"Conservative_Temperature\"] = pd.to_numeric(ts_all[\"Conservative_Temperature\"],errors='coerce')\n",
    "ts_all[\"Absolute_Salinity\"] = pd.to_numeric(ts_all[\"Absolute_Salinity\"])\n",
    "ts_all[\"Depth\"] = pd.to_numeric(ts_all[\"Depth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "49e0ac15-9c6c-4582-9e4b-17da01f926fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For profiles with multiple anomalies, if anomalies were within 100 m depth of each other, keep only the anomaly with a stronger AOU anomaly\n",
    "to_delete = [] # record which anomalies to delete (indexed by unique floatstation_anomdepth)\n",
    "\n",
    "for floatstation in anoms[anoms.duplicated('floatstation')]['floatstation']: # for each unique profile with detected anomalies\n",
    "    for i in anoms[anoms['floatstation']==floatstation].AOU_anomaly_depth:\n",
    "        for j in anoms[anoms['floatstation']==floatstation].AOU_anomaly_depth:\n",
    "            if i-j !=0 and np.abs(i-j) <100: # Do a pairwise comparison of depth for each anomaly. Look for ones that are <100m apart\n",
    "                floatstation_sorted = anoms[(anoms['floatstation']==floatstation)&(anoms['AOU_anomaly_depth'].isin([i,j]))].sort_values(by='AOU_anomaly',ascending=False) # Among those, sort them by AOU magnitude\n",
    "                to_delete.append(floatstation_sorted[0:1]['floatstation_anomdepth'].item()) # Select only the weakest one to delete\n",
    "\n",
    "                \n",
    "anoms = anoms[~anoms['floatstation_anomdepth'].isin(to_delete)] #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80a3e1b-b308-49a6-aff7-8add0ae7a81b",
   "metadata": {},
   "source": [
    "## Save dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d6b0c6f9-b286-46a8-b1b1-5f3de75baf73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save anoms and surface_bgc to pickles\n",
    "anoms.to_pickle('outputs/ESP_anomalies.pkl')\n",
    "surface_bgc.to_pickle('outputs/surface_mixedlayers.pkl')\n",
    "ts_all.to_pickle('outputs/ts_allprofiles.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db19cdcf-56fa-4ef3-8a1b-13debb1db37a",
   "metadata": {},
   "source": [
    "## Download FSLEs\n",
    "\n",
    "FSLEs downloaded from AVISO+: https://www.aviso.altimetry.fr/en/data/products/value-added-products/fsle-finite-size-lyapunov-exponents.html \\\n",
    "Delayed time ssalto/duacs product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f281eeee-2a19-4a90-97bb-4e24f6616593",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded = [i[12:-3] for i in glob.glob('fsles/fsles*.nc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08becdc3-5889-4a72-af70-791dec7bc05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a 1x1 degree grid cell around each float profile\n",
    "\n",
    "downloaded = [i[12:-3] for i in glob.glob('fsles/fsles*.nc')]\n",
    "\n",
    "for index,row in tqdm(surface_bgc[~(surface_bgc['floatstation'].isin(downloaded))&(surface_bgc['mon_day_yr']<='2023-06-07')].iterrows()):\n",
    "    floatstation = str(row['floatstation'])\n",
    "    x = f\"{row['Lon']:.3f}\"\n",
    "    y = f\"{row['Lat']:.3f}\"\n",
    "    xmin=f\"{row['Lon']-0.5:.3f}\"\n",
    "    xmax=f\"{row['Lon']+0.5:.3f}\"\n",
    "    ymin=f\"{row['Lat']-0.5:.3f}\"\n",
    "    ymax=f\"{row['Lat']+0.5:.3f}\"\n",
    "    datemin = str(row['mon_day_yr'].date())\n",
    "    datemax = str(row['mon_day_yr'].date())\n",
    "        \n",
    "    request = [\"python\", \"/Users/Michael/miniconda3/envs/spatial/lib/python3.8/site-packages/motuclient/motuclient.py\", \"-u\", \"mlc383@marine.rutgers.edu\", \"-p\", \"AQNeo9\",\\\n",
    "\"-m\", \"https://motu.aviso.altimetry.fr/motu-web/Motu\", \"-s\", \"AvisoFSLE\", \"-d\", \"dataset-duacs-dt-global-allsat-madt-fsle\",\\\n",
    "\"-x\", xmin, \"-X\", xmax, \"-y\", ymin, \"-Y\", ymax, \"-t\", datemin, \"-T\", datemax,\\\n",
    "\"--outputWritten\", \"netcdf\", \"-v\", \"fsle_max\",\\\n",
    "\"-o\", \"fsles/\", \"-f\", 'fsles_{}.nc'.format(floatstation)]\n",
    "    \n",
    "    output = subprocess.run(request, capture_output=True, text=True)\n",
    "    \n",
    "    downloaded = [i[12:-3] for i in glob.glob('fsles/fsles*.nc')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f94bb65-edb9-4306-9662-2446a01c20cb",
   "metadata": {},
   "source": [
    "### Add ASLEs to float dataframes\n",
    "For each float profile, calculate the strongest (highest magnitude) and mean FSLE values within a 1x1 degree grid cell \\\n",
    "This indicates whether the profile was in the vicinity of a strong submesoscale front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a88683-5946-4b69-8a14-97876f03d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded = [i[12:-3] for i in glob.glob('fsles/fsles*.nc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb982d9a-5723-4a9e-8efd-3c9a43879604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fsle_mean(floatstation): # Get the mean FSLE value\n",
    "    if floatstation in downloaded:\n",
    "        fsledb = xr.open_dataset('fsles/fsles_{}.nc'.format(floatstation))\n",
    "        mean_fsle = float(fsledb.fsle_max.mean().values)\n",
    "    else:\n",
    "        mean_fsle = np.nan\n",
    "        \n",
    "    return mean_fsle\n",
    "\n",
    "def get_fsle_min(floatstation): # Get the strongest (ie highest magnitude / most negative) FSLE value\n",
    "    if floatstation in downloaded:\n",
    "        fsledb = xr.open_dataset('fsles/fsles_{}.nc'.format(floatstation))\n",
    "        min_fsle = float(fsledb.fsle_max.min().values)\n",
    "    else:\n",
    "        min_fsle = np.nan\n",
    "        \n",
    "    return min_fsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0383dea-fce1-49be-a262-532e07ee9325",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_bgc['fsle_max_mean'] = surface_bgc.apply(lambda x: get_fsle_mean(x.floatstation),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dad16d5-9a4d-4502-bc99-5b2c8aa9b6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_bgc['fsle_max_min'] = surface_bgc.apply(lambda x: get_fsle_min(x.floatstation),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf1d73f-38d8-4eea-95d8-a03d513f0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_bgc['fsle_max_mean_minus01'] = np.abs(surface_bgc['fsle_max_mean']-0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6115610a-434c-4a1a-9a61-f20ac89668a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "anoms['fsle_max_mean'] = anoms.apply(lambda x: get_fsle_mean(x.floatstation),axis=1)\n",
    "anoms['fsle_max_min'] = anoms.apply(lambda x: get_fsle_min(x.floatstation),axis=1)\n",
    "anoms['fsle_max_mean_minus01'] = np.abs(anoms['fsle_max_mean']-0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c8d532-7255-4747-9130-999e390d2639",
   "metadata": {},
   "source": [
    "#### Re-save float dataframes with FSLE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a839d8-8b83-4895-a394-8c7a9272825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "anoms.to_pickle('outputs/ESP_anomalies.pkl')\n",
    "surface_bgc.to_pickle('outputs/surface_mixedlayers.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "toc-autonumbering": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
